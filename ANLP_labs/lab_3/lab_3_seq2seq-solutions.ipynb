{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDxS8RLD9Mf1"
   },
   "source": [
    "# Lab 3: Sequence-to-Sequence Modeling with Recurrent Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXwKFZc6D3gk"
   },
   "source": [
    "## Don't read this file until you've worked through the lab!\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "This file contains completed code and answers to questions. We provide it so you can check your answers or if you get stuck. But, you should make a real attempt to solve the questions on your own first.\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybdRfjvbDI4A"
   },
   "source": [
    "The objective of this lab is to _implement, train, and evaluate a recurrent neural network (RNN)-based sequence-to-sequence (seq2seq) model_. Well, that's a mouthful, so let's unpack the two key terms.\n",
    "\n",
    "A **Recurrent Neural Network** (RNN) is a type of neural network designed to handle sequences, like text or speech. It processes one item of the sequence at a time and carries a hidden state—a kind of short-term memory—that summarises the history of the sequence observed so far. The same weights are reused at each time step, letting it learn patterns over time.\n",
    "\n",
    "\n",
    "**Sequence-to-sequence learning** is a framework used for tasks in which both the input and the output are variable-length sequences that are not necessarily aligned, such as machine translation or summarization. Nowadays, in general-purpose AI models most tasks are framed as seq2seq, where both the input (e.g., task instructions and a question) and the output (e.g., a reasoning trace and an answer) are sequences.\n",
    "\n",
    "In this lab, we apply the seq2seq RNN to a toy task, the SCAN dataset, mapping a sequence of textual instructions into a sequence of actions. With all due differences, this is the stripped down version of commanding a robot!\n",
    "\n",
    "**Important**: Not all student pairs will be able to complete all sections and exercises of this lab: and that is fine! Most students should be able to reach the end of section 7 (\"Evaluating the model\"). If you do not manage to complete Section 8 (\"Out-of-distribution generalisation\") in time during the lab session, you are invited to revisit it at your own convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1vnik59EUTj"
   },
   "source": [
    "## What you will learn in this lab\n",
    "\n",
    "### Tools and practical issues:\n",
    "\n",
    "In this lab, you will learn:\n",
    "- how to preprocess and batch examples when they have different lengths\n",
    "- how to implement a sequence-to-sequence model\n",
    "- how to train this model through stochastic gradient descent\n",
    "- how to evaluate your model's predictions on a series of metrics, which measure exact match and overlap wrt a reference\n",
    "- to read the documentation of a library and use it\n",
    "\n",
    "### Concepts: sequence-to-sequence models and generalisation\n",
    "\n",
    "After working through the lab, you should be able to:\n",
    "- explain the different challenges in sequence to sequence modelling\n",
    "- understand the difference between i.i.d. and out-of-distribution generalisation\n",
    "   \n",
    "You should also understand more clearly:\n",
    "- how neural models are optimised\n",
    "- how choices in the neural model architecture affect its accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNY9VAhAVU3U"
   },
   "source": [
    "# 1. Install and import dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_sXQ-DY1T32"
   },
   "source": [
    "Let's first install and import the necessary dependencies. Most notably:\n",
    "\n",
    "\n",
    "\n",
    "*   `torch` (Pytorch), a library for implementing deep learning models, training them, and evaluating them.\n",
    "*   `wandb` (Weights & Biases), a library to track metrics from your experiments.\n",
    "\n",
    "_Optional: If you are not familiar with the other libraries, you can find the documentation for most of them on https://docs.python.org/3/library/_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "hT4G0wBlDJ54"
   },
   "outputs": [],
   "source": [
    "!pip install -q wandb\n",
    "!pip install -q evaluate\n",
    "!pip install -U -q datasets\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import evaluate\n",
    "\n",
    "# Tracking\n",
    "import wandb\n",
    "\n",
    "# Set random seeds to make our experiments (almost) deterministic\n",
    "# This makes it easier to reproduce results\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSAlkZ4lVZN7"
   },
   "source": [
    "# 2. Compositionality and the SCAN dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Dwh0JdxEn5X"
   },
   "source": [
    "**Compositional generalisation** is the ability to understand and produce a potentially infinite number of novel combinations from known components. This is the case of natural language, where infinite sentences can be construed from a finite number of words.\n",
    "\n",
    "For example, assume a person knows the meaning and usage of words such as \"*twice*\" and \"*again*\" and then learns a new (imaginary) verb such as \"*to dax*\". Through compositional generalisation, they can immediately understand or produce instructions such as \"*dax twice*\" or \"*dax again*\" even if they haven't encountered these specific combinations before.\n",
    "\n",
    "With the growing capabilities of machine learning models, researchers have become increasingly interested in testing whether such models can replicate this key aspect of human language.\n",
    "To support systematic investigation of this question, the **SCAN dataset** was introduced by [Lake and Baroni (2018)](https://github.com/brendenlake/SCAN/blob/master/README.md).\n",
    "\n",
    "SCAN consists of a set of commands and their corresponding action sequences. These are the actions an agent should perform to execute the commands successfully. The commands and actions are defined compositionally based on primitive verbs (\"*jump*\", \"*walk*\", \"*run*\", \"*turn*\", etc.), modifiers (\"*twice*\", \"*thrice*\", \"*left*\", etc.) and connectors (\"*and*\", \"*after*\", etc.). Here are some examples.\n",
    "\n",
    "|Command | Action sequence |\n",
    "| --- | --- |\n",
    "| IN: jump                |                       OUT: JUMP |\n",
    "| IN: jump left            |                       OUT:  TURN_LEFT JUMP |\n",
    "| IN: jump around right       |                   OUT: TURN_RIGHT JUMP TURN_RIGHT JUMP TURN_RIGHT JUMP TURN_RIGHT JUMP |\n",
    "| IN: turn left twice          |                  OUT: TURN_LEFT TURN_LEFT |\n",
    "| IN: jump thrice               |                 OUT: JUMP JUMP JUMP |\n",
    "| IN: jump opposite left and walk thrice   |      OUT: TURN_LEFT TURN_LEFT JUMP WALK WALK WALK |\n",
    "| IN: jump opposite left after walk around left | OUT: TURN_LEFT WALK TURN_LEFT WALK TURN_LEFT WALK TURN_LEFT WALK TURN_LEFT TURN_LEFT JUMP |\n",
    "\n",
    "In the first part of this lab, we are going to use a random split of the dataset (80% training, 20% test) that is i.i.d. (**independent and identically distributed**). This means that each example is independent from the others and that all examples are drawn from the same probability distribution. In this case, it means that the test set can be expected to exhibit the same properties as the train set.\n",
    "\n",
    "Let's first download the SCAN dataset files from the internet, then load them into a Python list, where each element is a tuple (input sequence, output sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tl_R_KQsDJbg"
   },
   "outputs": [],
   "source": [
    "# Download SCAN dataset\n",
    "!wget https://raw.githubusercontent.com/brendenlake/SCAN/refs/heads/master/simple_split/tasks_train_simple.txt -O train_simple.txt\n",
    "!wget https://raw.githubusercontent.com/brendenlake/SCAN/refs/heads/master/simple_split/tasks_test_simple.txt -O test_simple.txt\n",
    "\n",
    "def load_scan_file(file_path):\n",
    "    with open(file_path) as f:\n",
    "        data = [line.strip().split(\" OUT: \") for line in f.readlines()]\n",
    "    return [(inp.split()[1:], [x[2:] for x in out.split()]) for inp, out in data]\n",
    "\n",
    "train_data = load_scan_file(\"train_simple.txt\")\n",
    "test_data = load_scan_file(\"test_simple.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFCV4scbsug9"
   },
   "source": [
    "Let's explore some properties of the SCAN dataset.\n",
    "\n",
    "The input command is always a combination of the following words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X11MhsS-ihfj"
   },
   "outputs": [],
   "source": [
    "unique_primitives = set()\n",
    "for in_out in train_data:\n",
    "    unique_primitives.update(in_out[0])\n",
    "print(unique_primitives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbOy9BEcUVP5"
   },
   "source": [
    "...while the output is a combination of the following actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-kSPyqFTUbdL"
   },
   "outputs": [],
   "source": [
    "unique_actions = set()\n",
    "for in_out in train_data:\n",
    "    unique_actions.update(in_out[1])\n",
    "print(unique_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTjdfcx-9MIo"
   },
   "source": [
    "We will also define a helper function to print out specific examples from our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryT1zaqb9LNK"
   },
   "outputs": [],
   "source": [
    "def print_example(exm):\n",
    "    # for formatting the input and output when printing each sample\n",
    "    input, output = exm\n",
    "    print(f\"IN: {\" \".join(input)}\")\n",
    "    print(f\"OUT: {\" \".join(output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7D9_xXAKaDyg"
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "1. Can you work out the action sequence corresponding to the following commands? Take a look at the examples above to answer this.\n",
    "\n",
    "- IN: *jump opposite left*\n",
    "- IN: *walk around left*\n",
    "- IN: *run opposite right and look left*\n",
    "- IN: *look thrice after jump around right*\n",
    "\n",
    "2. How would you describe the \"algorithm\" you used to map between the tokens in the input sequence and the actions in the output sequence?\n",
    "\n",
    "3. What do you notice?\n",
    "\n",
    "  *   Is there a 1-to-1 mapping between words and actions?\n",
    "  *   Does the order of word spans always correspond to the order in which the corresponding actions appear?\n",
    "\n",
    "4. As a final exercise, assume that the model has seen the two following examples during training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3pzPTZd1VSN"
   },
   "outputs": [],
   "source": [
    "print_example(train_data[544])\n",
    "print_example(train_data[1043])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7E7ngDe6Giti"
   },
   "source": [
    "During evaluation, the model needs to execute the following command:\n",
    "\n",
    "IN: _jump opposite right after walk around right thrice_\n",
    "\n",
    "How does the test command relate to the two training commands? How can the model take advantage of this to generalise to this unseen command?\n",
    "\n",
    "After completing the exercises with your partner, verify your solutions below.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIpNZ2gDG6ST"
   },
   "source": [
    "#### Solution\n",
    "\n",
    "1. Instructions and corresponding actions:\n",
    "\n",
    "\n",
    "- OUT: TURN_LEFT TURN_LEFT JUMP\n",
    "- OUT: TURN_LEFT WALK TURN_LEFT WALK TURN_LEFT WALK TURN_LEFT WALK\n",
    "- OUT: TURN_RIGHT TURN_RIGHT RUN TURN_LEFT LOOK\n",
    "- OUT: TURN_RIGHT JUMP TURN_RIGHT JUMP TURN_RIGHT JUMP TURN_RIGHT JUMP LOOK LOOK LOOK\n",
    "\n",
    "2. You are implicitly parsing the word sequence identifying the distinct word classes (verbs, modifiers, connectors, etc.) and the syntactic structure that underlies them.\n",
    "\n",
    "3. There is no 1-to-1 mapping as one word can correspond to multiple actions (e.g., via repetitions like _thrice_). Moreover, the relative order of word spans and actions spans may be inverted (e.g., via temporal markers like _after_).\n",
    "\n",
    "4. This is a form of recombination. If the train data contains patterns \"A B\" and \"C D\", the model should be able to generalise to \"A D\". To know more, you can read \"[Compositionality decomposed: How do neural networks generalise](https://www.jair.org/index.php/jair/article/view/11674)?\" (Hupkes et al. 2020) for more information after the lab. The correct actions for the test example are the following:\n",
    "\n",
    "- IN: _jump opposite right after walk around right thrice_\n",
    "- OUT: TURN_RIGHT WALK TURN_RIGHT WALK TURN_RIGHT WALK TURN_RIGHT WALK TURN_RIGHT WALK TURN_RIGHT WALK TURN_RIGHT WALK TURN_RIGHT WALK TURN_RIGHT WALK TURN_RIGHT WALK TURN_RIGHT WALK TURN_RIGHT WALK TURN_RIGHT TURN_RIGHT JUMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Srx-y60Hlasp"
   },
   "source": [
    "# 3. Data preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcoJ_aJxwxc4"
   },
   "source": [
    "After familiarising ourselves with the SCAN dataset, we will develop a neural network model that can automatically take in a sequence of commands as input, and produce a sequence of actions as output.\n",
    "\n",
    "To use a neural network to perform this task, we first need to convert both the input and output tokens into a format that can be processed by a neural network, i.e., vector representations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4Q_T6pjzjT5"
   },
   "source": [
    "As you know from week 4, the most widespread approach is to assign each unique token a scalar index and use an `nn.Embedding` layer (from the `torch` library) to learn a dense vector representation for each token, also known as an *embedding*. We will talk more about `nn.Embedding` shortly, but let's focus on assigning token indices for now.\n",
    "\n",
    "## Exercise 2\n",
    "\n",
    "As an exercise, fill in the code for the function below, following the specifications in the function descriptor. Also, keep in mind that:\n",
    "- you should ensure that this mapping is deterministic. You can ensure this by sorting your vocabularies (words and actions) alphabetically.\n",
    "- you should reserve 3 indices for special tokens in both vocabularies (see below for an explanation of why this is needed): 0 for `<pad>` (padding), 1 for `<bos>` (beginning of sentence), and 2 for `<eos>` (end of sentence).\n",
    "\n",
    "_Hint_: First create sets of words and actions from the dataset examples, then construct the corresponding dictionaries. In Python, you can update the elements of a `set` with the `update` method and iterate through the indices and elements of a `list` through `enumerate`.\n",
    "\n",
    "Once you've populated the `build_vocab` function, the code uses the resulting vocabularies to map from words/actions to indices (and viceversa) via the `numericalize` function. Note that this function also maps the output sequence into a **tensor**. This is a `torch` class used to efficiently represent (and perform calculations on) numerical arrays of various dimensions and sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJ3O2_pVZmHk"
   },
   "outputs": [],
   "source": [
    "def build_vocab(pairs):\n",
    "    '''\n",
    "    This function takes in:\n",
    "    -  pairs: the list of (input, output) tuples in the training set\n",
    "    and returns four dictionaries:\n",
    "    - input_w2i: input word to index\n",
    "    - input_i2w: index to input word\n",
    "    - output_w2i: output word to index\n",
    "    - output_i2w: index to output word\n",
    "    '''\n",
    "    ################\n",
    "    # YOUR CODE HERE\n",
    "    ################\n",
    "\n",
    "input_w2i, input_i2w, output_w2i, output_i2w = build_vocab(train_data)\n",
    "\n",
    "def numericalize(seq, vocab):\n",
    "    # maps a token to its corresponding unique index\n",
    "    return torch.tensor([vocab[w] for w in seq])\n",
    "\n",
    "in_seq, out_seq = train_data[0]\n",
    "\n",
    "print('Original:')\n",
    "print('IN:', in_seq)\n",
    "print('OUT:', out_seq)\n",
    "\n",
    "print('Indices:')\n",
    "print('IN:', numericalize(in_seq, input_w2i))\n",
    "print('OUT:', numericalize(out_seq, output_w2i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6FvY_nJU9Bd"
   },
   "source": [
    "The three special tokens added to the input and output vocabularies have some special purposes:\n",
    "\n",
    "**\\<eos>**: marks the *end of sequence*. It tells the encoder when the input sequence is finished, and tells the decoder when to stop generating output.\n",
    "\n",
    "**\\<bos>**: marks the *beginning of sequence*. It is used only for the decoder side to signal the beginning of decoding.\n",
    "\n",
    "**\\<pad>**: used for *padding* sequences to a uniform length so that they can be processed in batches.\n",
    "\n",
    "*These tokens don’t appear in the original SCAN commands or actions, but they’re essential for training and evaluating sequence models properly.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhpn120CZbi6"
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6gQ_elZqUI2Z"
   },
   "outputs": [],
   "source": [
    "def build_vocab(pairs):\n",
    "    '''\n",
    "    This function takes in:\n",
    "    -  pairs: the list of (input, output) tuples in the training set\n",
    "    and returns four dictionaries:\n",
    "    - input_w2i: input word to index\n",
    "    - input_i2w: index to input word\n",
    "    - output_w2i: output word to index\n",
    "    - output_i2w: index to output word\n",
    "    '''\n",
    "    unique_primitives = set()\n",
    "    unique_actions = set()\n",
    "    for in_out in pairs:\n",
    "        unique_primitives.update(in_out[0])\n",
    "        unique_actions.update(in_out[1])\n",
    "\n",
    "    # word to index\n",
    "    input_w2i = {w: i+3 for i, w in enumerate(sorted(unique_primitives))}\n",
    "    output_w2i = {w: i+3 for i, w in enumerate(sorted(unique_actions))}\n",
    "    input_w2i[\"<pad>\"] = 0\n",
    "    input_w2i[\"<bos>\"] = 1\n",
    "    input_w2i[\"<eos>\"] = 2\n",
    "    output_w2i[\"<pad>\"] = 0\n",
    "    output_w2i[\"<bos>\"] = 1\n",
    "    output_w2i[\"<eos>\"] = 2\n",
    "\n",
    "    # revert the two dictionaries to obtain index to word\n",
    "    input_i2w = {i: w for w, i in input_w2i.items()}\n",
    "    output_i2w = {i: w for w, i in output_w2i.items()}\n",
    "    return input_w2i, input_i2w, output_w2i, output_i2w\n",
    "\n",
    "\n",
    "input_w2i, input_i2w, output_w2i, output_i2w = build_vocab(train_data)\n",
    "\n",
    "def numericalize(seq, vocab):\n",
    "    # maps a token to its corresponding unique index\n",
    "    return torch.tensor([vocab[w] for w in seq])\n",
    "\n",
    "in_seq, out_seq = train_data[0]\n",
    "\n",
    "print('Original:')\n",
    "print('IN:', in_seq)\n",
    "print('OUT:', out_seq)\n",
    "\n",
    "print('Indices:')\n",
    "print('IN:', numericalize(in_seq, input_w2i))\n",
    "print('OUT:', numericalize(out_seq, output_w2i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KV_Sb_PrS5Ec"
   },
   "source": [
    "# 4. Dataloader and batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScZ40wd7a9XF"
   },
   "source": [
    "Next, let's define `SCANDataset`, a custom Python class to represent our **dataset** (inheriting from `torch.utils.data.Dataset`). Given a sample index, the dataset will return an input-output sequence mapped to the corresponding token indices via `numericalize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ne6N3A6KVpg_"
   },
   "outputs": [],
   "source": [
    "class SCANDataset(Dataset):\n",
    "    def __init__(self, data, input_vocab, output_vocab):\n",
    "        self.data = data\n",
    "        self.input_vocab = input_vocab\n",
    "        self.output_vocab = output_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp, out = self.data[idx]\n",
    "        input_seq = numericalize(inp + ['<eos>'], self.input_vocab)\n",
    "        target_seq = numericalize(out + ['<eos>'], self.output_vocab)\n",
    "        return input_seq, target_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMWN7UbNRl5c"
   },
   "source": [
    "**Batching** is the process of grouping multiple examples together and processing them in parallel during training.\n",
    "Instead of updating the model parameters after seeing just one example at a time, we update them after computing the average loss across a batch of examples.\n",
    "This leads to faster training and more stable gradient estimates.\n",
    "\n",
    "However, in sequence-to-sequence tasks, input and output sequences often have variable lengths.\n",
    "This makes batching challenging, because vectors in a batch must be the same shape in order to stack them together into a matrix.\n",
    "This is where `collate_fn` comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaevumDUR0L0"
   },
   "source": [
    "`collate_fn` is a function that specifies how to combine individual examples into a batch. It is passed to the `DataLoader`, and it handles:\n",
    "\n",
    "- Padding sequences to the same length, which is the maximum length of any sequence in that batch (so they can be stacked into a matrix).\n",
    "\n",
    "- Keeping track of original lengths, which is important for models like RNNs to avoid wasting computation on pad tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQDyXZKIRxBG"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    # we need to keep track of the lengths of the input sequences\n",
    "    input_lengths = [len(seq) for seq in inputs]\n",
    "    inputs = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=input_w2i[\"<pad>\"])\n",
    "    targets = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=output_w2i[\"<pad>\"])\n",
    "    return inputs, targets, input_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qjl_ZOw9eB5I"
   },
   "source": [
    "We then use `collate_fn` to create instances of dataloaders for both the train and test splits. It is worth noting that:\n",
    "\n",
    "- The value of `BATCH_SIZE` is usually a power of 2. The choice represents a trade-off between high values (high memory load, more stable gradients) and low values (low memory load, less stable gradients).\n",
    "- We shuffle the training data loader to make sure that each batch is a random sample. Technical note: this guarantees that the gradient estimate we obtain on a single batch of examples is unbiased with respect to the true gradient for the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXkU0xbs1B8j"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = SCANDataset(train_data, input_w2i, output_w2i)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "test_dataset = SCANDataset(test_data, input_w2i, output_w2i)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXv2S8AbUzLl"
   },
   "source": [
    "Here are a few example batches from the train_loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UisBFsIzSVe5"
   },
   "outputs": [],
   "source": [
    "inputs, targets, input_lengths = next(iter(train_loader))\n",
    "\n",
    "print(\"Input tensor (padded) has shape:\", inputs.shape, \"\\n\")\n",
    "print(f'This means there are {inputs.shape[0]} training samples (input sequences), with all the sequences padded to {inputs.shape[1]} tokens as the max length\\n')\n",
    "print('Below is the first batch of input sequences:\\n')\n",
    "print(inputs, \"\\n\")\n",
    "print('(recall 0 is the index for the <pad> token)\\n')\n",
    "print(\"Actual lengths of the 32 input sequences:\", input_lengths, \"\\n\")\n",
    "print(\"Output tensor (padded) has shape:\", targets.shape, \"\\n\")\n",
    "print(f'This means there are {targets.shape[0]} training samples (output sequences), with all the sequences padded to {targets.shape[1]} tokens as the max length\\n')\n",
    "print('Below is the first batch of output sequence:\\n')\n",
    "print(targets, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cu8G5w74cu_w"
   },
   "source": [
    "# 5. Sequence-to-sequence model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VusYKtOqLGSd"
   },
   "source": [
    "Let's start implementing in `torch` our recurrent model, which consists of an encoder and a decoder:\n",
    "\n",
    "- the **encoder** maps the input sequence of words into a sequence of hidden representations.\n",
    "- the **decoder** maps the sequence of hidden representations into a sequence of actions.\n",
    "\n",
    "We will implement a kind of recurrent network called a Gated Recurrent Unit (GRU; [Cho et al. 2014](https://arxiv.org/pdf/1406.1078)). In comparison with vanilla RNNs, GRUs have learnable gates that stabilise learning by avoiding the risk of gradient vanishing and explosion.\n",
    "\n",
    "The encoder (and the decoder) will consist of a class inheriting from `nn.Module` (via `super().__init__()`): this way, this class will automatically keep track of trainable parameters and their gradients. Both classes contain two methods:\n",
    "\n",
    "`__init__`: This method defines the architecture of the model by specifying each layer within the model. Each layer is initialized with the appropriate input and output dimensions (and possibly other keyword arguments):\n",
    "\n",
    "- An `Embedding` layer maps token indices (with `vocab_size` as the size of the vocabulary) to dense vector representations with dimensionality `hidden_size`.\n",
    "\n",
    "- In the `GRU` layer, the first `hidden_size` indicates the size of each input vector (which is the output of the embedding layer). The second `hidden_size` specifies the size of the GRU's hidden state. These two values are identical in our implementation, but they don't necessarily have to be the same. `num_layers=2` stacks two `GRU` layers, allowing the model to capture more complex temporal dependencies.\n",
    "\n",
    "\n",
    "`forward`: This method outlines the computation that occurs when data is passed through the layers within the model to produce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0CLd8077rVKd"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,\n",
    "                                      hidden_size)\n",
    "        self.rnn = nn.GRU(input_size=hidden_size,\n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=2,\n",
    "                          batch_first=True)\n",
    "\n",
    "    def forward(self, input, lengths):\n",
    "        # input shape: [batch_size, seq_len]\n",
    "        embedded = self.embedding(input)\n",
    "        # embedded shape: [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        # See note below on packing\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded,\n",
    "            lengths,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        x, hidden = self.rnn(packed)\n",
    "        # hidden shape: [num_layers, batch_size, hidden_size]\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1pL4MRZd74M"
   },
   "source": [
    "## Note on Packing\n",
    "\n",
    "RNNs, by default, will process the padding tokens as a normal input token unless told otherwise — which wastes computation and add noise to the training.\n",
    "\n",
    "`pack_padded_sequence` tells the RNN to skip the padding tokens and only compute over the actual (unpadded) content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DvlCEWhavGF"
   },
   "source": [
    "## Exercise 3\n",
    "\n",
    "Now implement the `__init__` method of the `Decoder`. Remember to add:\n",
    "\n",
    "- an `Embedding` layer, to embed the sequence of decoded output actions.\n",
    "- 2 `GRU` layers.\n",
    "- a `Linear` layer (used only in the decoder), which projects vectors of dimensionality `hidden_size` back to `vocab_size` to map hidden states to output actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FC5rONVW_0z9"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ################\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # input: [batch_size, 1]\n",
    "        embedded = self.embedding(input)  # [batch_size, 1, hidden_size]\n",
    "        output, hidden = self.rnn(embedded, hidden)  # [batch_size, 1, hidden_size]\n",
    "        output = self.out(output.squeeze(1))  # [batch_size, vocab_size]\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwMH1kxa_yWd"
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0l4-2wECPFtL"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,\n",
    "                                      hidden_size)\n",
    "        self.rnn = nn.GRU(input_size=hidden_size,\n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=2,\n",
    "                          batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size,\n",
    "                             vocab_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # input shape: [batch_size, 1]\n",
    "        embedded = self.embedding(input)\n",
    "        # embedded shape: [batch_size, 1, hidden_size]\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        # hidden shape: [batch_size, 1, hidden_size]\n",
    "        output = self.out(output.squeeze(1))\n",
    "        # output shape: [batch_size, vocab_size]\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mIpYQhijaRV"
   },
   "source": [
    "# 6. Training the model\n",
    "\n",
    "We can create instances of an encoder and a decoder with the following specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8edgbCYFaetP"
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 200\n",
    "\n",
    "example_encoder = Encoder(len(input_w2i), HIDDEN_SIZE)\n",
    "example_decoder = Decoder(len(output_w2i), HIDDEN_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HufQmsnSjtLH"
   },
   "source": [
    "We can check the layers inside the encoder and the decoder with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F34t3264joOZ"
   },
   "outputs": [],
   "source": [
    "example_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhcxyr3kjnSs"
   },
   "outputs": [],
   "source": [
    "example_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqmW6576z_j-"
   },
   "source": [
    "We are now ready to train our GRU encoder-decoder on SCAN!\n",
    "\n",
    "We will use `wandb` to help us keep track of the important metrics, such as the training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZpZYi-tU305d"
   },
   "outputs": [],
   "source": [
    "def train_model(run_name, num_epochs, learning_rate, hidden_units=200):\n",
    "    # Initialize wandb to track the experiment metrics\n",
    "    wandb.init(project=\"scan-seq2seq\", name=run_name)\n",
    "\n",
    "    # Initialise your encoder-decoder GRU\n",
    "    encoder = Encoder(len(input_w2i), hidden_units)\n",
    "    decoder = Decoder(len(output_w2i), hidden_units)\n",
    "    wandb.watch(encoder)\n",
    "    wandb.watch(decoder)\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = SCANDataset(train_data, input_w2i, output_w2i)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    # Use the Adam optimizer and a cross-entropy loss\n",
    "    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=learning_rate)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=output_w2i[\"<pad>\"])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        # For every example,\n",
    "        for inputs, targets, lengths in tqdm(loader):\n",
    "            # remove any gradients currently stored in the optimizer\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # encode the input into a sequence of hidden states\n",
    "            encoder_output = encoder(inputs, lengths)\n",
    "\n",
    "            # initialize the decoder input as a BOS token for the entire batch\n",
    "            decoder_input = torch.full(\n",
    "                (targets.size(0),),\n",
    "                output_w2i[\"<bos>\"],\n",
    "                dtype=torch.long,\n",
    "            ).unsqueeze(1)\n",
    "\n",
    "            # use the last hidden state of the encoder to initialize the hidden state of the decoder\n",
    "            decoder_hidden = encoder_output\n",
    "\n",
    "            # for every time step,\n",
    "            loss = 0\n",
    "            for t in range(targets.size(1)):\n",
    "                # decode an action\n",
    "                output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                # calculate the loss\n",
    "                loss += loss_fn(output, targets[:, t])\n",
    "                # update the decoder input via \"teacher forcing\"\n",
    "                decoder_input = targets[:, t].unsqueeze(1)\n",
    "\n",
    "            # perform a step of gradient descent by\n",
    "            # 1) calculating the gradients for all trainable parameters via backpropagation\n",
    "            loss.backward()\n",
    "            # 2) updating the parameter values based on the gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # track the total loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        wandb.log({\"loss\": total_loss / len(loader)})\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    # return the trained encoder and decoder\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZDykUvVaBn8"
   },
   "source": [
    "Now we can run the `train_model` function.\n",
    "Note that usually, we would save the **checkpoint** of a model (i.e., the model state including its current parameter values) every few epochs during training; however, here we only return the model state after final epoch in this case as both the model and the data are quite small.\n",
    "\n",
    "When running the code below, you will be prompted to log into Weights & Biases (`wandb`), and will need to create an account if you don't already have one.\n",
    "Follow the instructions to copy and paste the API key: after verifying your email, make sure to select \"Academic\" when creating your account. Skip the creation of an organization profile, then choose \"Models\" when asked \"What do you want to try first?\".\n",
    "\n",
    "Each training run would take around 5-10 minutes to run on a CPU.\n",
    "While the model is running, you should start solving Exercise 4.1, 4.2, and 4.3 below. From time to time, check the loss logged during the training run at the link following `View run at`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TK8Wxh7HumO0"
   },
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "lr = 0.001\n",
    "gru_encoder, gru_decoder = train_model(f\"{n_epochs}-{lr}\", n_epochs, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kb2q0_SObfK0"
   },
   "source": [
    "## Exercise 4\n",
    "\n",
    "1. Which different hyperparameter settings could you explore to further enhance model performance in our code, in terms of optimization and model architecture?\n",
    "2. In the code above, we are backpropagating after recurring across the entire target sequence (rather than after each step): which ramifications may this have for the model's gradients?\n",
    "3. The `train_model` function uses \"teacher forcing\". Can you explain what teacher forcing is and why it is used in training sequence-to-sequence models?\n",
    "4. Based on the loss profile, do you think we need to train more epochs? What other information can you use to decide when to stop training?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQQ1GqqVQnvD"
   },
   "source": [
    "### Solution\n",
    "\n",
    "1. Hyperparameters to consider:\n",
    "- optimization wise: learning rate, batch size. As a rule of thumb, higher learning rates speed up training but make finding a good local optimum harder. Think of it this way: if you're hiking and looking for the lowest point of a valley, it would be easier if you take human steps than giant leaps. Larger batch sizes instead reduce the variance of the gradient estimate (hence, stabilise training) but come at a higher cost in terms of time and memory.\n",
    "- architecture wise: Number of layers, number of hidden units. Both increasing the depth (number of layers) and width (number of hidden units) of a neural model makes it more expressive (informally, more suitable to find the right class of solutions) but also less efficient.\n",
    "\n",
    "2. This may lead to vanishing or exploding gradients. However, this is mitigated by the fact that we are using a *gated* recurrent unit.\n",
    "\n",
    "3. The crucial line is `decoder_input = targets[:, t].unsqueeze(1)`: instead of passing the decoder's generated `output` as the next decoder input, we pass the token at the corresponding position in the gold-truth `targets`. This makes the training process more robust as models can recover from early mistakes in the rest of the sequence.\n",
    "\n",
    "4. Training loss profile:\n",
    "- GRU loss drops fast at first, then starts plateauing around the 4th epoch, but continues decreasing. In principle, it could be decreased further, but possibly at the cost of **overfitting**: this happens when your model memorises the training data instead of extracting the underlying information, hence losing the ability to generalise to new (test) data.\n",
    "- A common approach to avoid overfitting is to monitor the loss on a validation set to perform \"**early stopping**\": checkpoint the model state after every epoch, evaluate it on the evaluation set, and only retain the checkpoint with the lowest validation loss across epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXdcPTUh01Iw"
   },
   "source": [
    "# 7. Evaluating the model\n",
    "\n",
    "We can evaluate the performance of the trained sequence-to-sequence model using both qualitative and quantitative methods:\n",
    "1. **Qualitative Evaluation**.\n",
    "\n",
    "We inspect the model’s predictions on selected test examples to assess whether it produces outputs that are:\n",
    "\n",
    "- Syntactically well-formed (e.g., following the correct command structure)\n",
    "\n",
    "- Semantically correct (e.g., executing the intended action)\n",
    "\n",
    "This manual analysis helps reveal specific patterns in the model’s successes and failures, such as whether it correctly handles modifiers like \"*twice*\" or \"*around*\" right.\n",
    "\n",
    "2. **Quantitative Evaluation**.\n",
    "\n",
    "We use standard automatic metrics to assess performance across the test set:\n",
    "\n",
    "- Exact Match (Accuracy):\n",
    "The proportion of test examples for which the model’s entire output sequence exactly matches the ground-truth (aka gold-standard or target) sequence.\n",
    "\n",
    "- n-gram overlap (BLEU):\n",
    "Calculates how many short subsequences (n-grams) overlap between the prediction and the ground truth, capturing partial correctness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "P86C8-LhubCR"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(encoder, decoder, dataset, max_len=50):\n",
    "    sequences_to_evaluate = []\n",
    "\n",
    "    # load test data\n",
    "    data_loader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # switch to evaluation mode, which disables train-time behaviors like dropout\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # for each test example:\n",
    "    for inputs, targets, lengths in tqdm(data_loader):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # encode the input sequence\n",
    "            encoder_output = encoder(inputs, lengths)\n",
    "\n",
    "            # initialize the decoder input as a BOS token\n",
    "            decoder_input = torch.tensor(\n",
    "                [[output_w2i[\"<bos>\"]]],\n",
    "            )  # shape (1,1)\n",
    "\n",
    "            # use the last hidden state of the encoder to initialize the hidden state of the decoder\n",
    "            decoder_hidden = encoder_output\n",
    "\n",
    "            predicted_tokens = []\n",
    "\n",
    "            # for every time step,\n",
    "            for _ in range(max_len):\n",
    "                # get the most likely next action\n",
    "                output_logits, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                top1 = output_logits.argmax(1).item()\n",
    "\n",
    "                # stop generation when an EOS token is generated by the model\n",
    "                if top1 == output_w2i[\"<eos>\"]:\n",
    "                    break\n",
    "\n",
    "                predicted_tokens.append(output_i2w[top1])\n",
    "\n",
    "                # update decoder input\n",
    "                decoder_input = torch.tensor([[top1]])\n",
    "\n",
    "            # get ground truth sequence (remove <eos> and <pad>)\n",
    "            reference_tokens = [\n",
    "                output_i2w[idx.item()]\n",
    "                for idx in targets[0]\n",
    "                if idx.item() not in (output_w2i[\"<eos>\"], output_w2i[\"<pad>\"])\n",
    "            ]\n",
    "\n",
    "            sequences_to_evaluate.append((predicted_tokens, reference_tokens))\n",
    "\n",
    "    return sequences_to_evaluate\n",
    "\n",
    "def exact_match(sequences):\n",
    "    total_exact_match = 0\n",
    "\n",
    "    for predicted_tokens, reference_tokens in sequences:\n",
    "      total_exact_match += int(predicted_tokens == reference_tokens)\n",
    "\n",
    "    exact_match_accuracy = total_exact_match / len(sequences)\n",
    "    print(f\"Exact match accuracy: {exact_match_accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjRhdqXks7iB"
   },
   "source": [
    "Now let's get the model predictions and evaluate their exact match with the corresponding references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9i-V4pNB1B8o"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gru_encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sequences \u001b[38;5;241m=\u001b[39m evaluate_model(\u001b[43mgru_encoder\u001b[49m, gru_decoder, test_dataset, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m      2\u001b[0m exact_match(sequences)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gru_encoder' is not defined"
     ]
    }
   ],
   "source": [
    "sequences = evaluate_model(gru_encoder, gru_decoder, test_dataset, max_len=50)\n",
    "exact_match(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFiSOeqApxnc"
   },
   "source": [
    "## Exercise 5\n",
    "\n",
    "1. Can you analyse the errors the RNN is making in a qualitative way? Review a dozen pairs of `sequences` where there is no exact match and check what's wrong with the output sequences.\n",
    "2. Exact match is excessively strict as it requires that the actions sequences are identical. Another more fine-grained evaluation metric would compute the n-gram overlap between the target and predicted action sequences. Use [BLEU from `evaluate`](https://huggingface.co/spaces/evaluate-metric/bleu) to implement this metric and evaluate the model predictions. Make sure to read the (short) documentation in the link. *Hints*: you will need to `load` the metric, prepare the predictions and references in the right format, then `compute` the metric on these sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HusxHaycXvVh"
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mEH0Ip02o-1"
   },
   "source": [
    "1. By inspecting the errors, you fill find (for instance) that predicted sacstion sequences tend to be shorter than the target ones. In other words, the model terminates generation too early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3GXKghFs16Yf"
   },
   "outputs": [],
   "source": [
    "def print_wrong(sequences):\n",
    "    count = 0\n",
    "    for predicted_tokens, reference_tokens in sequences:\n",
    "        if predicted_tokens != reference_tokens:\n",
    "            print(f\"Predicted: {predicted_tokens}\\nReference: {reference_tokens}\")\n",
    "            count += 1\n",
    "        if count == 12:\n",
    "          break\n",
    "\n",
    "print_wrong(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zjo7TKJ5XzOd"
   },
   "source": [
    "2. The BLEU metric can be evaluated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SzH72HiWKOjf"
   },
   "outputs": [],
   "source": [
    "def evaluate_bleu(sequences):\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    all_predictions = [\" \".join(x) for (x, y) in sequences]\n",
    "    all_references = [\" \".join(y) for (x, y) in sequences]\n",
    "\n",
    "    bleu_results = bleu.compute(predictions=all_predictions, references=all_references)\n",
    "    print(f\"BLEU score: {bleu_results['bleu']:.3f}\")\n",
    "\n",
    "evaluate_bleu(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2RKYiidcZfJ"
   },
   "source": [
    "# 8. Out-of-distribution (OOD) generalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4F6pcmLgNhKV"
   },
   "source": [
    "Now we look at whether the GRU model can generalize outside of the training domain. For this, we look at a different training-test split, where the output sequences in the test set are all longer than the ones in the training set. This is a case of **out-of-distribution (OOD) generalization**, where the test data are not from the same distribution as the train data (hence, they are not i.i.d.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-00cIBOlPq2"
   },
   "outputs": [],
   "source": [
    "# Download SCAN dataset\n",
    "!wget https://raw.githubusercontent.com/brendenlake/SCAN/refs/heads/master/length_split/tasks_train_length.txt -O train_length.txt\n",
    "!wget https://raw.githubusercontent.com/brendenlake/SCAN/refs/heads/master/length_split/tasks_test_length.txt -O test_length.txt\n",
    "\n",
    "new_train_data = load_scan_file(\"train_length.txt\")\n",
    "new_test_data = load_scan_file(\"test_length.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qxjtu3UT5GMs"
   },
   "source": [
    "Let's inspect the distributions of lengths in the training and test sets of this data split to study how they differ. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HG9Iojj58Vo_"
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_length_distribution(data, title, sequence_type='output'):\n",
    "    if sequence_type == 'input':\n",
    "        lengths = [len(in_seq) for in_seq, _ in data]\n",
    "    elif sequence_type == 'output':\n",
    "        lengths = [len(out_seq) for _, out_seq in data]\n",
    "    else:\n",
    "        raise ValueError(\"sequence_type must be either 'input' or 'output'\")\n",
    "\n",
    "    length_counts = Counter(lengths)\n",
    "    total_examples = len(data)\n",
    "    length_proportions = {length: count / total_examples for length, count in sorted(length_counts.items())}\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(length_proportions.keys(), length_proportions.values())\n",
    "    plt.xlabel(\"Sequence Length\")\n",
    "    plt.ylabel(\"Proportion of Examples\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_length_distribution(new_train_data, \"Training Set Output Length Distribution\", sequence_type='output')\n",
    "\n",
    "plot_length_distribution(new_test_data, \"Test Set Output Length Distribution\", sequence_type='output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cSgpuXNBWOg"
   },
   "source": [
    "## Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kcIZe61Nd9P"
   },
   "source": [
    "Using the existing code, can you\n",
    "- create loaders for the new training and test set reusing `SCANDataset`\n",
    "- train a new GRU model on the new train set reusing `train_model`\n",
    "- generate predicted sequences for the new GRU model reusing `evaluate_model`\n",
    "- code a new function to measure to how the model performs on target sequences grouped by length in terms of exact match and BLEU.\n",
    "\n",
    "Then answer the following questions:\n",
    "- What effect do you expect shifts in distribution to have on model performance? Compare evaluation metrics of your experiments on o.o.d and i.i.d. generalisation.\n",
    "- Is it realistic to expect these shifts in distribution in realistic settings where models are trained on natural language data?\n",
    "- Is it realistic to expect compositional generalisation to be sufficient to model pairs of inputs and outputs in natural language?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ai7Icr5NO1RE"
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zeu_A6RfNNSW"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "new_train_dataset = SCANDataset(new_train_data, input_w2i, output_w2i)\n",
    "new_train_loader = DataLoader(new_train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "new_test_dataset = SCANDataset(new_test_data, input_w2i, output_w2i)\n",
    "new_test_loader = DataLoader(new_test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0FWyhPDVcnt"
   },
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "lr = 0.001\n",
    "new_gru_encoder, new_gru_decoder = train_model(f\"{n_epochs}-{lr}-OOD\", n_epochs, lr)\n",
    "new_sequences = evaluate_model(new_gru_encoder, new_gru_decoder, new_test_dataset, max_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qA5KFJI5ViGJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def exact_match_per_length(sequences):\n",
    "    length2em = defaultdict(list)\n",
    "\n",
    "    for predicted_tokens, reference_tokens in sequences:\n",
    "        exact_match = int(predicted_tokens == reference_tokens)\n",
    "        ref_len = len(reference_tokens)\n",
    "        length2em[ref_len] += [exact_match]\n",
    "\n",
    "    # Compute accuracy per output length\n",
    "    for length in sorted(length2em.keys()):\n",
    "        acc = sum(length2em[length]) / len(length2em[length])\n",
    "        print(f\"Length {length:2d} | Accuracy: {acc:.3f}\")\n",
    "\n",
    "exact_match(new_sequences)\n",
    "exact_match_per_length(new_sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FI91Wi74w5cf"
   },
   "source": [
    "1. You should find that the model achieves a lower accuracy on longer sequences, being unseen/rare.\n",
    "2. Generalising to longer lengths is still an open problem in natural language processing, including for state of the art large language models. Naturalistic long sequences are harder to collect and put a strain on training. Due to the open-ended nature of language, there can always be a longer sequence to model at test time.\n",
    "3. Although compositional generalization plays an important role in human cognition, natural language is not always strictly compositional.\n",
    "In the SCAN task, commands like “_jump twice_” remain interpretable when the primitive “_jump_” is replaced with another of the same kind, such as “_look_” or “_run_.”\n",
    "However, natural language also includes many idiomatic expressions, where the meaning of the whole cannot be derived from its parts.\n",
    "For example, the phrase “_jump the gun_” means to act prematurely, a meaning not predictable from the individual words.\n",
    "In this case, replacing “_jump_” with another verb, as in “_run the gun_”, does not yield a valid or meaningful expression.\n",
    "Thus to imitate humans' capacity in processing languages, models must be capable of handling both compositional and idiomatic aspects of language."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
