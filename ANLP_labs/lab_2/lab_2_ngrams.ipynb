{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb1b3c7",
   "metadata": {},
   "source": [
    "# Lab 2: Language Identification with character n-gram models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47003a10-0782-4c77-a8ee-b7a491533084",
   "metadata": {},
   "source": [
    "## Character n-gram models: what and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f8813e-8562-424a-be7c-73838bdeb47c",
   "metadata": {},
   "source": [
    "In lecture, we talked about n-gram models over **words**, but it's also possible to build n-gram models over **characters**. These can actually be useful for certain tasks, like identifying what language a text is written in.\n",
    "\n",
    "It's much easier (and requires less data) to build an n-gram model over characters than over words, so will do that here, to help you build your intuitions about this type of model.\n",
    "\n",
    "You will work with data from three different languages to build and explore character-level n-gram models for a simple language identification task. Along the way, you’ll confront issues like rare characters, smoothing, underflow, and generation strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e54d4c-a288-40c2-8a8e-6c073eda9afa",
   "metadata": {},
   "source": [
    "## What you will learn in this lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca3df46-c9ef-46eb-abae-5ae94e12bee3",
   "metadata": {},
   "source": [
    "### Tools and practical issues: \n",
    "\n",
    "In this lab, you will learn:\n",
    "- how to easily split data into training and development subsets using the `datasets` library.\n",
    "- how to use these splits, together with a separate test set, to correctly to tune hyperparameters and test generalization.\n",
    "- some possible pitfalls to watch out for in your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba068a0-0593-42cc-9131-668e31ffe758",
   "metadata": {},
   "source": [
    "### Concepts: n-gram models and language identification\n",
    "\n",
    "After working through the lab, you should be able to:\n",
    "- compute the probability of a sequence given an n-gram model\n",
    "- explain how to generate new sentences using an n-gram language model\n",
    "- explain how to use a character n-gram model to do language identification\n",
    "   \n",
    "You should also understand more clearly:\n",
    "- how n-gram models are trained\n",
    "- the effects of smoothing and other hyperparameter choices on the model's behaviour (both generation and perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ebce01",
   "metadata": {},
   "source": [
    "## 1. Loading and splitting the data\n",
    "\n",
    "Today you’ll use another dataset we uploaded to Hugging Face. It contains sentences in three languages widely spoken in South Africa: \n",
    "- **English:** a language in the West Germanic branch of the Indo-European language family with significant influences from Old French. It is spoken as a first language by around 380 million people worldwide, with another billion second-language speakers.\n",
    "- **Afrikaans:** another language in the West Germanic branch of the Indo-European language family. It evolved from Dutch starting around the 17th century, and is spoken as a first language by around 7 million people, mainly in South Africa and nearby regions.\n",
    "- **Xhosa: (or isiXhosa):** a language in the Nguni branch of the Bantu language family. It is closely related to Zulu and is spoken as a first language by around 8 million people, mainly in South Africa and nearby regions.\n",
    "\n",
    "👉 Run the next two cells to load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c3f0041-9cef-4401-86b2-0024a4e311aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.12/site-packages (2.2.1)\n",
      "Collecting datasets\n",
      "  Using cached datasets-4.1.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Using cached pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.12/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.7.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.33.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /opt/conda/lib/python3.12/site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached datasets-4.1.1-py3-none-any.whl (503 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "Installing collected packages: pyarrow, multiprocess, datasets\n",
      "\u001b[2K  Attempting uninstall: pyarrow\n",
      "\u001b[2K    Found existing installation: pyarrow 20.0.0\n",
      "\u001b[2K    Uninstalling pyarrow-20.0.0:\n",
      "\u001b[2K      Successfully uninstalled pyarrow-20.0.0━━━\u001b[0m \u001b[32m0/3\u001b[0m [pyarrow]\n",
      "\u001b[2K  Attempting uninstall: multiprocess━━━━━━━━━━━━\u001b[0m \u001b[32m0/3\u001b[0m [pyarrow]\n",
      "\u001b[2K    Found existing installation: multiprocess 0.70.18m0/3\u001b[0m [pyarrow]\n",
      "\u001b[2K    Uninstalling multiprocess-0.70.18:━━━━━━\u001b[0m \u001b[32m0/3\u001b[0m [pyarrow]\n",
      "\u001b[2K      Successfully uninstalled multiprocess-0.70.1832m0/3\u001b[0m [pyarrow]\n",
      "\u001b[2K  Attempting uninstall: datasets[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [multiprocess]\n",
      "\u001b[2K    Found existing installation: datasets 2.2.1━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [multiprocess]\n",
      "\u001b[2K    Uninstalling datasets-2.2.1:[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [multiprocess]\n",
      "\u001b[2K      Successfully uninstalled datasets-2.2.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [multiprocess]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [datasets]2/3\u001b[0m [datasets]\n",
      "\u001b[1A\u001b[2KSuccessfully installed datasets-4.1.1 multiprocess-0.70.16 pyarrow-21.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# once again, we need to update `datasets`\n",
    "%pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed475497",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Our variable names are based on the ISO 639-3 language codes for the languages in the dataset.\n",
    "# These are common language codes used in NLP, but you may also see two-character ISO 639-1 codes commonly.\n",
    "# However, ISO 639-3 is more comprehensive and includes many languages not covered by the two-character codes!\n",
    "eng = load_dataset('EdinburghNLP/south-african-lang-id', 'english')['train']\n",
    "xho = load_dataset('EdinburghNLP/south-african-lang-id', 'xhosa')['train']\n",
    "afr = load_dataset('EdinburghNLP/south-african-lang-id', 'afrikaans')['train']\n",
    "\n",
    "# By using a dictionary, you can associate each dataset with its language code.\n",
    "# Do you see why we use the code as the key and not the dataset itself?\n",
    "# This allows to write code that can operate on any number of languages without changing the code structure.\n",
    "# And it makes it easy to keep track of which dataset corresponds to which language.\n",
    "# This is a common pattern in NLP and other data processing tasks.\n",
    "corpora = {'eng': eng, 'xho': xho, 'afr': afr}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565bbb49",
   "metadata": {},
   "source": [
    "In this lab, you'll be training models, adjusting hyperparameters, and evaluating them. To evaluate your changes and avoid over-fitting, you'll use the *train--dev--test* splitting paradigm discussed in class. \n",
    "\n",
    "You might have noticed that when we loaded the dataset from HuggingFace, we selected a key called `'train'`. That's because you always have to select a split before you can iterate over a HuggingFace dataset, and HuggingFace interprets data without any splits as being a single `'train'` split. \n",
    "\n",
    "However, we are now going to split this data to create the actual training and development sets, so that you can explore and tune hyperparameters on the development set. \n",
    "\n",
    "The `datasets` library has a function we can use to easily split off a smaller subset of the data for development, but note that it will call this subset `'test'`!! Despite the name, this split will *not* be used for the test set in this lab. We have set aside another dataset for that, which we will introduce toward the end of the lab.\n",
    "\n",
    "By default, `train_test_split` selects a *random* subset of the data to split off, which can be useful if there might be differences between the early and late parts of the dataset, and you don't want this to affect your training. However, if you're trying to compare directly to (your own or someone else's) previous work, you need to be careful about whether you split the data the same way. \n",
    "\n",
    "**Further reading for later:** If you're new to Hugging Face, they provide useful tutorials with much more information about Datasets and other libraries, which you can find [here](https://huggingface.co/docs/datasets/index).\n",
    "\n",
    "👉 Run the next two cells to load the data and see what its structure is. Then, change the final line in the second cell to print out the first 10 sentences in each language. Do you see anything that surprises you? (You might or might not, remember the data splits are random!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39e8ae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = {\n",
    "    lang: dataset.train_test_split(test_size=0.1) # this puts 10% of the data into a \"test\" set (which we will use for development)\n",
    "    for lang, dataset in corpora.items() # see how organizing the data in a dictionary allows us to write less code?\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e985ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### eng data: ###\n",
      "diimido oxalic acid dihydrazine ;\n",
      "Constitutional Court \n",
      "( d ) the antiplant agents listed in annexure f , whether in substantially pure form or in a mixture ;\n",
      "( iv ) the indigenous biological resources to which a permit relates may not be sold , donated or transferred to a third party without the written consent of the minister .\n",
      "application forms\n",
      "procedures for dealing with complaints about judicial officers ; \n",
      "cas numbers are shown to assist in identifying whether a particular chemical or mixture is controlled , irrespective of nomenclature .\n",
      "letter from the bank stating all signatories\n",
      "Application of international law \n",
      "\n",
      "### xho data: ###\n",
      "Ngaphandle kokujikelez ' ezikolweni ezisezilalini kwiphulo lam ekuthiwa nguNozincwadi , ndiza kube ndibalisa ePhilippines ngasekupheleni kwalo nyaka .\n",
      "Kubaluleke kakhulu ukondla imfuyo yakho ngethuba lasebusika ukuze ingayilahli imeko yayo .\n",
      "* Unokufaka ibango ukuba ubukhe wachaphazeleka kwingozi njengomqhubi , umkhweli okanye umhambi ngeenyawo .\n",
      "Inqaku lentengiso\n",
      "USamkeliswe ukholelwa ekubeni kufuneka ubenendlela yokuphatha yaye uzimisele ukufunda ukuze ubeneshishini elizinzileyo .\n",
      "Ukhulile ngoku kwaye akasakhathazwa zezo zinto .\n",
      "114 Buz ' igqwetha : Iingcebiso zomthetho\n",
      "Kungabangelwa nakukuba nekholesteroli eninzi okanye ihayi-hayi - ngxaki ezo zichaphazela ukuya kwegazi kwilungu lobudoda okukokona kubalulekileyo !\n",
      "Abanye baneziqinisekiso zemfundo ezaneleyo kodwa abanawo amava omsebenzi , nto leyo ebaluleke kakhulu emsebenzini wokufama .\n",
      "\n",
      "### afr data: ###\n",
      "' n geldige btw-sertifikaat ( waar toepaslik ) .\n",
      "moet 'n provinsie aanbevelings doen aan die kabinetslid wat vir polisiëring verantwoordelik is.\n",
      "bioprospektering\n",
      "technical note :\n",
      "variola virus ;\n",
      "id-dokumente van eienaars\n",
      "beslote korporasies ( bk 's ) moet ' n samewerkingsooreenkoms aanheg .\n",
      "kode : kode : .\n",
      "f. polytetrahydrofuran polyethylene glycol ( tpeg ) .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's take a quick look at the data.\n",
    "for lang, dataset in corpora.items():\n",
    "    print(f'### {lang} data: ###')\n",
    "    lines = dataset['train']['text'][1:10]\n",
    "    for line in lines: \n",
    "        print(line) \n",
    "    print(\"\")\n",
    "    #print(f'the first 10 train item: {dataset['train'][:10]}\\n')\n",
    "    #print(f'the first 10 train item: {dataset['test'][:10]}\\n')\n",
    "    # This line should help you see the structure of the datasets.\n",
    "    # Replace it with code to print the first ten lines from the training set of each language!\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d4806a-1a02-476a-ab91-7e367ce732b2",
   "metadata": {},
   "source": [
    "## 2. Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be979181-102a-4703-96be-f0d43abf79f6",
   "metadata": {},
   "source": [
    "If we directly train models on this data and try to evaluate their perplexity on held out data, we might encounter errors. That's because there may be rare characters that we don't see in training. (This would be a much bigger problem if our model was over words, but can be a problem even for characters.) We want our code to generalize to unseen characters, so we can still assign probabilities to sequences that contain them. \n",
    "\n",
    "We will take a standard approach and replace all rare and unseen characters with a shared unknown character: `�` (the Unicode character for an unknown character). This allows our model to assign some probability to unseen characters and also prevents it overfitting to rare characters which are coincidentally only present in one of the corpora. \n",
    "\n",
    "👉 **THINK:** What does this imply about the probabilities of different rare or unseen characters? How is it similar to add-alpha smoothing?\n",
    "assign equally probabilities to every unseen characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18948761-c260-4031-84a7-ba9a392ed2bd",
   "metadata": {},
   "source": [
    "Some languages do contain characters that don't occur in others! For instance, English doesn't use `क`, `ã` or `角`.\n",
    "\n",
    "👉 **THINK:** Do you think an English n-gram model *should* assign zero probability to strings containing these characters? Why or why not? Can you think of any contexts where they might appear in English text?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97de7e1-15d4-4459-8ab6-640890bd213e",
   "metadata": {},
   "source": [
    "In order to remove rare characters, we need to decide what counts as \"rare\". Normally, you would take a look at your data statistics and perhaps try a few different frequency thresholds (tuning on a development set).\n",
    "\n",
    "In this case, we've done some of that for you already and we are just going to use a threshold of 500. But you should still sanity-check the results by looking at what is getting removed.\n",
    "\n",
    "👉 Run the next two cells to identify the rare characters and print them out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51ee13ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def find_rare_chars(corpora, threshold=500):\n",
    "    \"\"\" find characters that occur fewer than 'threshold' times across all corpora\n",
    "    and return them as an alphanumerically sorted string \"\"\"\n",
    "\n",
    "    # count character frequencies in each corpus\n",
    "    counters  = []\n",
    "    for _, corpus in corpora.items():#eng\n",
    "        counts = Counter()\n",
    "        for text in corpus['train']:\n",
    "            counts.update(text['text'])#character frequency\n",
    "        counters.append(counts)\n",
    "\n",
    "    all_chars = set([char for counter in counters for char in counter.keys()])\n",
    "\n",
    "    # characters that occur less than 500 times across all three texts:\n",
    "    rare_chars = [char for char in all_chars if sum([freq[char] for freq in counters]) < threshold]\n",
    "    return ''.join(sorted(rare_chars)) # sorting alphnumerically to make output a bit easier to scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbde31dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!%*FHJQVWXYZ[]_°±µºÊËÏáèéêíóöú–﻿\n"
     ]
    }
   ],
   "source": [
    "rare_chars = find_rare_chars(corpora)\n",
    "print(rare_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b007266d-60b7-43b1-b615-acc44f54d56f",
   "metadata": {},
   "source": [
    "👉 **THINK:** Look at the rare characters. Do you think any of these characters shouldn't be replaced with the unknown character? If so, why not? ！%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5343447d-be9b-4eda-92e1-16f0a8f06889",
   "metadata": {},
   "source": [
    "👉 **CHALLENGE QUESTION:** We know that encountering unseen characters in test data will cause problems. But why don't we simply deal with that at test time, by replacing the unseen characters with `�`? That is, why did we use `�` to replace characters that *did* occur in training, but rarely? (There are actually a few reasons, some more subtle than others. Don't spend more than a couple of minutes thinking about this now, you can come back to it later.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751f1dd5",
   "metadata": {},
   "source": [
    "👉 Now, run the next cell to actually replace the rare characters with the unknown character using a regular expression. (You'll learn more about these in CPSLP if you're taking it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0794b565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67825e03fecd442b8ee56ecab06a29f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d1536acc104607a79d22f86096f423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93748cfe8d3349b1b3836efdbdc9d864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c814716e55b440219aa5b5193d7fe341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d9e46c4d70415fb1632b3f28bcd376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "913318add7124f5abb2310b488d278ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "corpora = {\n",
    "\tlang: corpus.map(\n",
    "\t\tlambda x: {'text': re.sub(f\"[{re.escape(rare_chars)}]\", '�', x['text'])}, # This regular expression identifies any character in rare_chars so we can replace it with the unknown character\n",
    "\t)\n",
    "\tfor lang, corpus in corpora.items()\n",
    "}  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f8d660",
   "metadata": {},
   "source": [
    "## 3. Defining the n-gram model and looking at test cases\n",
    "\n",
    "Now that we've preprocessed our corpora, it's time to build our n-gram model. We've written it so that our model works for different values of $N$. It also implements add-alpha smoothing. \n",
    "\n",
    "👉 **THINK:** Why do we need smoothing, even after replacing unknown characters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e765ce5-c5df-4426-a0ef-83628623d19d",
   "metadata": {},
   "source": [
    "The code for the n-gram model is below. It's probably not a good use of the lab session time to go through every line right now, but we encourage you to review it in more detail later. \n",
    "\n",
    "👉 For now, just make sure you understand the role of each function at a high level. Then, run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1c7f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class CharNGramLM:\n",
    "    def __init__(self, N=3, alpha=0.01):\n",
    "        self.N = N\n",
    "        if alpha < 0:\n",
    "            raise ValueError(\"Invalid value of alpha!\")\n",
    "        else:\n",
    "            self.alpha = alpha\n",
    "        # dictionary to hold counts of n-grams\n",
    "        # where keys are n-1 character tuples (context)\n",
    "        self.context_counts = defaultdict(\n",
    "            Counter\n",
    "        )  # a defautltdict allows us to create a new Counter for each new n-1-gram automatically\n",
    "        if N > 1:\n",
    "            self.vocab = set([\"<s>\", \"</s>\"])\n",
    "        elif N == 1:\n",
    "            self.vocab = set([\"</s>\"])  # don't need BOS\n",
    "        else:\n",
    "            raise ValueError(\"Invalid value of N!\")\n",
    "\n",
    "    def train(self, corpus):\n",
    "        ''' Given a corpus of sentences, store the counts of all character N-grams in the corpus.'''\n",
    "        for sentence in corpus:\n",
    "            # add start and end tokens\n",
    "            sentence = [\"<s>\"] * (self.N - 1) + list(sentence) + [\"</s>\"]\n",
    "            # update the counts of each n-gram\n",
    "            for i in range(len(sentence) - self.N + 1):\n",
    "                context = tuple(sentence[i : i + self.N - 1])\n",
    "                char = sentence[i + self.N - 1]\n",
    "                self.context_counts[context][char] += 1\n",
    "                self.vocab.add(char)\n",
    "\n",
    "    def print_counts(self):\n",
    "        ''' Print out the counts of all N-grams that have non-zero counts, alphnumerically ordered.'''\n",
    "        for context, counts in sorted(self.context_counts.items()):\n",
    "            print(f\"Context {context}:\")\n",
    "            for char, count in sorted(counts.items()):\n",
    "                print(f\"   C({char!r} | {context}) = {count}\")\n",
    "\n",
    "    def print_probs(self):\n",
    "        ''' For each context in alphanumeric order, print out the conditional probability\n",
    "        of each character in the vocabulary (including zero probabilities).'''\n",
    "        for context, counts in sorted(self.context_counts.items()):\n",
    "            print(f\"Context {context}:\")\n",
    "            for char in sorted(self.vocab):\n",
    "                if char != \"<s>\" : # We never generate <s> following another character, so it's not in the conditional probabilities\n",
    "                    prob = self.prob(context, char)\n",
    "                    print(f\"   P({char!r} | {context}) = {prob}\")\n",
    "\n",
    "    # We've included the next two functions so it's easier for you to check\n",
    "    # correctness on tiny examples, but we don't normally use raw\n",
    "    # probabilities in models because they can underflow for very long\n",
    "    # sequences. In this case, we do the computation in log space to\n",
    "    # make sure it will always be right, and this function should also\n",
    "    # work for reasonable sequence lengths.\n",
    "    def prob(self, context, char):\n",
    "        '''Returns the smoothed probability of char in the given context'''\n",
    "        return 2 ** self.logprob(context, char)\n",
    "\n",
    "    def prob_seq(self, sentence):\n",
    "        '''Returns the  probability of the sentence, according to the model'''\n",
    "        return 2 ** self.logprob_seq(sentence)\n",
    "\n",
    "    def logprob(self, context, char):\n",
    "        '''Returns the smoothed log probability of char in the given context'''\n",
    "        context = tuple(context)\n",
    "        counts = self.context_counts[context]\n",
    "        if self.N == 1:\n",
    "            V = len(self.vocab)\n",
    "        else:\n",
    "            V = len(self.vocab) - 1 # Only count characters we might generate next, which doesn't include <s>\n",
    "        if (self.alpha == 0) and (counts[char] == 0):\n",
    "            return -math.inf # Negative infinity\n",
    "        else:\n",
    "            prob = (counts[char] + self.alpha) / (sum(counts.values()) + self.alpha * V)\n",
    "            return math.log2(prob)\n",
    "\n",
    "    def logprob_seq(self, sentence):\n",
    "        '''Returns the log probability of the sentence, according to the model'''\n",
    "        sentence = [\"<s>\"] * (self.N - 1) + list(sentence) + [\"</s>\"]\n",
    "        # replace OOV characters with a placeholder\n",
    "        sentence = [char if char in self.vocab else \"�\" for char in sentence]\n",
    "        score = 0.0\n",
    "        for i in range(len(sentence) - self.N + 1):\n",
    "            context = tuple(sentence[i : i + self.N - 1])\n",
    "            char = sentence[i + self.N - 1]\n",
    "            score += self.logprob(context, char)  # add the log probs of each n-gram (multiply the raw probabilities)\n",
    "        return score\n",
    "\n",
    "    def perplexity(self, corpus):\n",
    "        '''Returns the perplexity of the model on the given corpus'''        \n",
    "        length = 0\n",
    "        log_prob_sum = 0.0\n",
    "        for sentence in corpus:\n",
    "            log_prob_sum += self.logprob_seq(sentence)\n",
    "            length += len(sentence) + 1  # account for </s>\n",
    "        return 2 ** (-log_prob_sum / length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f0fe5b-9d2c-4c7d-8b10-b1a42a59595d",
   "metadata": {},
   "source": [
    "Never assume that code is correct without testing it, whether it was written by you, us, or someone else (including AI)!\n",
    "\n",
    "Before you train the model on the real data, you should check that it works correctly on a very small test case where you can check the results by hand. This will also help ensure that *you* understand how to compute the right result.\n",
    "\n",
    "(It's not always possible to check results this way! Sometimes you'll need to think of other ways to test code, and ideally you can write automated test cases.)\n",
    "\n",
    "We've constructed one such test case below.\n",
    "\n",
    "👉 Before you run this test case, compute the counts, probabilities, and perplexity by hand. Remember that for a sequence of length $L$, the perplexity is $2^{(-1/L)*log P(seq)}$. You'll need to consider how the begin/end of sequence markers figure into the computations! \n",
    "\n",
    "👉 Now run the test code below to check that your answers match the output of our model. If they don't, is there a bug in the model or in your own understanding?\n",
    "\n",
    "👉 We started by checking the unigram model without smoothing, because it's the simplest, but it's important to ensure the code also works for other cases. Again **by hand**, \n",
    "1. Figure out what counts and probabilities you should you get from this corpus if you use a bigram model (still with alpha = 0).\n",
    "2. Do you expect the training and testing perplexities to be higher or lower than with the unigram model? Why?\n",
    "\n",
    "👉 Now set N=2 and re-run the test code to check your answers. Do you see where smoothing becomes necessary? Update the value of alpha, rerun the code, and check that this change does what you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f162269-5c8e-4a2d-9f27-751e8dad481c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context ():\n",
      "   C('</s>' | ()) = 3\n",
      "   C('a' | ()) = 1\n",
      "   C('h' | ()) = 3\n",
      "   C('i' | ()) = 2\n",
      "Context ():\n",
      "   P('</s>' | ()) = 0.3333333333333333\n",
      "   P('a' | ()) = 0.11111111111111109\n",
      "   P('h' | ()) = 0.3333333333333333\n",
      "   P('i' | ()) = 0.22222222222222218\n",
      "0.02469135802469135\n",
      "PPL on train: 3.709\n",
      "PPL on test: 3.528\n"
     ]
    }
   ],
   "source": [
    "# Tiny toy corpus\n",
    "tiny_train = ['hi', 'ha', 'hi']\n",
    "tiny_test = ['hi','i']\n",
    "tiny_lm = CharNGramLM(N=1,alpha=0)\n",
    "tiny_lm.train(tiny_train)\n",
    "tiny_lm.print_counts()\n",
    "tiny_lm.print_probs()\n",
    "print(tiny_lm.prob_seq('hi'))\n",
    "print (f\"PPL on train: {tiny_lm.perplexity(tiny_train):.4}\")\n",
    "print (f\"PPL on test: {tiny_lm.perplexity(tiny_test):.4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3d84c5-b1bf-4037-8493-6f19dad02c76",
   "metadata": {},
   "source": [
    "## 4. Training on real corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67d0e1e-743f-4629-80b0-4a5f34f13b66",
   "metadata": {},
   "source": [
    "Character n-gram models can be used for language identification by training models on different languages. Then, when you get a piece of text, you check its perplexity under each model. Whichever model has the lowest perplexity is identified as the language of the text.\n",
    "\n",
    "Systems based on this idea are efficient and often work well, but they can still make errors! \n",
    "\n",
    "Let's take a look at what kinds of examples might present problems for this sort of model, using a simple trigram model with default smoothing.\n",
    "\n",
    "👉 Run the two cells below to train models on each of our three languages and compute the perplexities of some test sentences that could occur in English. Is the perplexity always lowest for the English model? If not, what sorts of input seem to cause problems for this way of doing language ID?\n",
    "\n",
    "Feel free to add your own test sentences to the list to explore this question further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ca78088-3188-44ea-840e-4a135ddb73f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a character n-gram language model for each language.\n",
    "lms = {}\n",
    "for lang, corpus in corpora.items():\n",
    "    lm = CharNGramLM(N=3)\n",
    "    lm.train(corpus['train']['text'])\n",
    "    lms[lang] = lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34cabaff-aa26-440c-98eb-a7b85e97879e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I love natural language processing.\n",
      "  eng ppl: 15.7190\n",
      "  xho ppl: 49.6789\n",
      "  afr ppl: 16.9916\n",
      "Sentence: An incomplete sentence.\n",
      "  eng ppl: 16.1419\n",
      "  xho ppl: 31.7597\n",
      "  afr ppl: 20.7150\n",
      "Sentence: marketing and sales operations\n",
      "  eng ppl: 7.2647\n",
      "  xho ppl: 40.3849\n",
      "  afr ppl: 9.6745\n",
      "Sentence: Pierre Vinken, chairman of Elsevier, is well-known in NLP for appearing in the first sentence of the WSJ corpus.\n",
      "  eng ppl: 22.4657\n",
      "  xho ppl: 50.5135\n",
      "  afr ppl: 33.6297\n",
      "Sentence: Hi\n",
      "  eng ppl: 274.3369\n",
      "  xho ppl: 125.0679\n",
      "  afr ppl: 137.3167\n",
      "Sentence: no\n",
      "  eng ppl: 27.5816\n",
      "  xho ppl: 121.6028\n",
      "  afr ppl: 344.9383\n",
      "Sentence: See?\n",
      "  eng ppl: 450.1264\n",
      "  xho ppl: 180.2554\n",
      "  afr ppl: 646.5665\n",
      "Sentence: Aha, Lycketoft.\n",
      "  eng ppl: 359.5674\n",
      "  xho ppl: 226.7015\n",
      "  afr ppl: 265.2185\n",
      "Sentence: 3601\n",
      "  eng ppl: 251.9075\n",
      "  xho ppl: 33.5477\n",
      "  afr ppl: 235.4080\n",
      "Sentence: hey @sloppyjoe wassup #chillin #fridaynight\n",
      "  eng ppl: 105.5258\n",
      "  xho ppl: 126.6744\n",
      "  afr ppl: 66.2655\n"
     ]
    }
   ],
   "source": [
    "my_test_sentences = ['I love natural language processing.',\n",
    "                     'An incomplete sentence.',\n",
    "                     'marketing and sales operations',\n",
    "                     'Pierre Vinken, chairman of Elsevier, is well-known in NLP for appearing in the first sentence of the WSJ corpus.',\n",
    "                     'Hi',\n",
    "                     'no',\n",
    "                     'See?',\n",
    "                     'Aha, Lycketoft.', # Lycketoft was one of the words in English Europarl with count=1\n",
    "                     '3601',\n",
    "                     'hey @sloppyjoe wassup #chillin #fridaynight']\n",
    "for sentence in my_test_sentences:\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    for lang, lm in lms.items():\n",
    "        print(f\"  {lang} ppl: {lm.perplexity([sentence]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f620a01-f6dc-4d1a-9733-aa4979c55cf5",
   "metadata": {},
   "source": [
    "## 5. Generating from language models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddba6ad-3ecc-4da8-9719-66c3bca4c902",
   "metadata": {},
   "source": [
    "This section is a small digression from the language ID task, to help you build intuitions about *generating* from language models. \n",
    "\n",
    "**If you are running short on time** and want to focus on language ID, you can skip this section and come back to it later. Before going to Section 4, you will need to **run the first two cells below.**\n",
    "\n",
    "Below, we've provided some code that implements several different generation (decoding) strategies. The default is just to sample from the language model probabilities (standard generation), but it also implements top-$k$ and temperature-scaled sampling. We will only explore top-$k$ sampling here, but if you want you can look at temperature-scaled sampling on your own after the lab.\n",
    "\n",
    "To better see how the output is affected by both $N$ and the sampling  method, we'll generate from models with different values of $N$.\n",
    "\n",
    "👉 Run the next two cells to train English models with different values of $N$ and implement generation. Then scroll down to the next question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "527114d4-5609-4dde-85d2-ba7aa3890c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train some models on English with different N\n",
    "eng_lms = {}\n",
    "for N in [1, 3, 5, 10]:\n",
    "    lm = CharNGramLM(N=N)\n",
    "    lm.train(corpora['eng']['train']['text'])\n",
    "    eng_lms[N] = lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "500e617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def generate(model, top_k=0, temperature=1, max_len=100):\n",
    "    '''Given a language model, use it to generate a single sequence. \n",
    "    Generation will stop when </s> is generated, or after max_len chars (whichever comes first).\n",
    "    If top_k = 0, we sample from the full distribution, otherwise re-normalize the top k choices and sample from those.\n",
    "    '''\n",
    "    context = ['<s>'] * (model.N - 1)\n",
    "    result = []\n",
    "    for _ in range(max_len):\n",
    "        nlog_probs = []\n",
    "        chars = []\n",
    "        for c in model.vocab:\n",
    "            log_p = model.logprob(context, c) / temperature\n",
    "            nlog_probs.append(-log_p)\n",
    "            chars.append(c)\n",
    "        \n",
    "        if top_k > 0:\n",
    "            pairs = sorted(zip(chars, nlog_probs), key=lambda x: x[1])[:top_k]\n",
    "            chars, nlog_probs = zip(*pairs)\n",
    "            probs = [math.exp(-log_p) for log_p in nlog_probs]\n",
    "        else:\n",
    "            exp_probs = [math.exp(-log_p) for log_p in nlog_probs]\n",
    "            total = sum(exp_probs)\n",
    "            probs = [p / total for p in exp_probs]\n",
    "\n",
    "        next_char = random.choices(chars, weights=probs if top_k > 0 else exp_probs)[0]\n",
    "        if next_char == '</s>':\n",
    "            break\n",
    "        result.append(next_char)\n",
    "        context = context[1:] + [next_char]\n",
    "    return ''.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b2fc44-0fba-45ad-901b-6f4e3448a8fa",
   "metadata": {},
   "source": [
    "👉 The following cell is going to generate 10 sequences (up to 100 characters each) from each of the four English $N$-gram models, which have $N$=\\[1, 3, 5, 10\\]. \n",
    "1. Roughly speaking, what do you think the difference will be between the output of the four models? Run the code to find out if you are correct.\n",
    "2. (**Challenge question**) Now take a closer look at the output. Do you see any places where the 5-gram or 10-gram model suddenly started generating total junk? (If not, you might need to rerun the code to get some new sequences. You should see this eventually.) Can you explain why that happened? \n",
    "3. So far we used $k=0$, which actually turns off top-k sampling (so, we sample from the full distribution when generating each character). What do you think will happen if you change $k$ to be 1, and how might that change the output of each model? (*Hint*: What character will be chosen at each generation step?) Try it to see if you're right! k=1 certainty/repeat\n",
    "4. If you want, you can try other values of $k$ to see whether you can find a good balance between generating diverse outputs and not generating junk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cdc8570-6252-478c-950d-903a418a7e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Generating with N = 1 , k = 5 *******\n",
      "ii((LLTL-T(LLiT-(TL(LL(iLi(iT-L(TLT((i-T-LiiT(-(i-TL-(i---TLiT(T-i(i-i-L(i-TTL-LTiTiiTiTT-iT-T-TTL-T\n",
      " Li(LTiLL(T--TT-L(i-iLiTLT(i((iiiLTT(L(-TT((iL-TiT-iii(i-iiLiTLTT--((-LiL--iiT-T-i((T-i(LT-L-TLLT((L\n",
      "oL-L-iTi---T(iiTTTiL(((iT((LiiLLLTLL(LTTL-Ti(iiLL-i-TT(i-iTT-ii(iiT-T(-TLi((i(LLTTTLi(iTT-LLTiT-T-i-\n",
      "i-TL(TL---(-LLTL(iLiTL(-LLL-(iiT(T-T--i--LLiLLTT((i--LiL(-((TTi(TiTLiiL(L-L-T-i(TL(TTi-L-TLi-i(T-LTT\n",
      "eLTLT(T((L-LTTTLL--L(-TLTi(ii((LTLiTL(T(-(ii-Ti--TTLLL(T(LLL(-(LTi-(-((LT-i(iLi((i(-T--i--(Tiii-T-(L\n",
      "iLii(T-iLTi-(T-L(-L--i--(-iiT(TiL-L-Li-T(-(iii((iTi-(T(-(Ti-T(-TL-L-LLiL(--i(((-T-(--iiTiTi(L--(L-L-\n",
      " T(LLi((iTL-((T--iT-L--LL((-iTLiL(T(-i(iT(((LLL-LL-(i(Li-T(LiiLTL-T(i(T-(i-i((-((iT-LT(Lii--Li-L-iT(\n",
      "e--Ti-Li-LTLii(L(--i(LTi---LTLLTi-LL(-TLL(iiT(-LT--(iT(T-iTT(--(-(-(L(-LLLLTT-((iiiLiTLL(TiL(TLTL-((\n",
      "iiT((T-Li-L--(-T--T((L-TL(TTLiiL-ii(-LTi(-TT(T--LT---iLL(LTLTiLi-LiTL(iTL((T((i(LTT-L((-TiT(i(T(ii--\n",
      "o--LL----i-L(-TiiTiii(--((i-T(TTii-L-i(TLT-iT(((iiiL(-(iL-i((iiLi(Ti-iii-(L(Tiii-LiTiLLi(i-i(iLL(i-L\n",
      "******* Generating with N = 3 , k = 5 *******\n",
      "andan tor an specif the persoures te the provined or any a mis ing the or the of and of the thounleg\n",
      "a ressions thares the per applicatity and ing , and thignal logencipart th in and the and ine commin\n",
      "te con th teduct of the pections of a mayste andent ition of the a son thor to the the or andamement\n",
      "incilles a recipme provincille cont , ing thed the con the prold ing an and in the a provistrom proc\n",
      "the any in suction a proproving the or the cons offies in to th ing and the propy orin any of th the\n",
      "( in the con of act of subsects of the publegislationtration the to con of and or as of thand ber as\n",
      "( cons as ing or mation the the com the the presse cons implor monal le th and the and th or of se a\n",
      "a provented of spen a proms of the pencludiss of the to and ithe thant int the a subject ing of the \n",
      "therect th the and ber thication assignis and is ort ass of the provires , ing approvin seares of th\n",
      "ification thand incil the a re pect the apose the of the com ine pubst ing or the a providenter to t\n",
      "******* Generating with N = 5 , k = 5 *******\n",
      "a permit is simulation , processary of postallation of the administration , and allocation of the pr\n",
      "a provincials for the present , includes present of the provincial legislation of the act , it was t\n",
      "the as for the new Constitution and to be ented by the presearch ;\n",
      "the capture and the province with the product of the systems special legislation , the act of the pr\n",
      "indigenous Constitution . \n",
      "a compoundar resolution of the application , or modified in partment and provincial technology , to \n",
      "the provincial council address to be council of the proported in the new Constitution , the public s\n",
      "anyone of the province of port a total addressing partment or a person contaminister the accord in t\n",
      "technology , and a directors such goods listice in annexure of the preview to a spectional legislati\n",
      "issuing agreement or a bsa ) an agreement , in terms of the press ) and to the cas 1274-08-4 )\n",
      "******* Generating with N = 10 , k = 5 *******\n",
      "it is the rental housing tribunal will conduct its business currently the case , nozzle inlets , cas\n",
      "the premier of a provincial legislation , the affected a total of 177,457 people ( laborators join t\n",
      "the permit relates may not be requester ( other than any international legislatures \n",
      "the administration of the bill or the rules mentioned in such application for an integrated export o\n",
      "the spread of the disease for two reasons :\n",
      "( 1 ) an application forms\n",
      "( a ) in terms of section 13 ( 2 ) ( b ) of the act , determined by them , and interest , including \n",
      "and provincial constitution or an Act of Parliament may pass a constitution or an Act of Parliament \n",
      "the proposed methodology ;\n",
      "and the applicant affiliated to and sign the agreement must be construed as a reference to- \n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "for N, lm in eng_lms.items():\n",
    "    print(\"******* Generating with N =\", N, \", k =\", k, \"*******\")\n",
    "    for _ in range(10):\n",
    "        print(generate(lm, top_k=k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eab39da-ef82-4586-a3df-90a39dcf0533",
   "metadata": {},
   "source": [
    "👉 **THINK:** If we want to get the best performance from our language ID system, we will need to tune the model hyperparameters (as we'll do in the next section). Should we include $k$ in the hyperparameters that we tune? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab6c7ba-fcec-4ad0-a391-3dbd1966f1ee",
   "metadata": {},
   "source": [
    "## 6. Exploring the effects of $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3a1568-ab93-4928-affe-b45c4c56a8fa",
   "metadata": {},
   "source": [
    "Now, let's see how the smoothing parameter affects the generated text and the model perplexity.\n",
    "\n",
    "For generation, we  will use regular sampling ($k=0$). Our code uses a 5-gram model, but you can try other values of $N$ later if you want.\n",
    "\n",
    "Notice that we are exploring $\\alpha$ values over several orders of magnitude. This is a common pattern in NLP and machine learning, especially if you don't have a good idea of the range that will work well. If you find huge differences between the results, you can always try more values in between the big jumps.\n",
    "\n",
    "👉 Run the code and look at the generated output. \n",
    "1. How does the value of alpha affect the generation quality? alpha++ random++\n",
    "2. Will the alpha that yields the best quality output also have the lowest perplexity on the development set? Why or why not?\n",
    "3. Now fill in the code to actually compute the perplexities on the training and development parts of the English data. Was your prediction correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60df0552-7149-4851-9f2a-4412c321ea36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Model with N = 5 , alpha = 1 *******\n",
      "perplexity on the English train data:4.26295\n",
      "perplexity on the English dev data:4.55797\n",
      "** Generated text: **\n",
      "I?zi:qbenq/.0EI5cjAm\n",
      "the replyR:.,/;RNxfIdKmgMygA65xtTcT3 7A06j(x\n",
      "the commercise of sport and ho�oLRr08o.yD2P\n",
      "any\n",
      "c. m8kSk,ëz:RrjrgzN;g.wLE2o60M9(x93kEmo�5:2;\n",
      "( iO-,:7Up�gibKL261.lRj.3pr4C'I,ejlPpfux?, a4g�c,UvgrGMfo8KKqc7jpCfPK7v3j)98:83Gwio5Npu:Dn;;y6wm;M9i\n",
      "blackc4b\n",
      "software stantity of the legislature of the province t/dkzg; dmcKeG9h0,;amfa7hx9(6hy Bh5dsBA/\n",
      "in AEPGUT'5B1rM1lrBRM8f\n",
      "( a ) must susMxj4:GbgltL5PTwi)laOC'gowt6Rd8cEMUkjwkA1MtG1?jxI-\n",
      "******* Model with N = 5 , alpha = 0.01 *******\n",
      "perplexity on the English train data:2.55108\n",
      "perplexity on the English dev data:2.79210\n",
      "** Generated text: **\n",
      "three to the in pass details ( b ) and the next elective in the who seekers or any in and all region\n",
      "in the minister the 2009 .\n",
      "if yes , liability of itself to the countries and opports or in the amendation of permit application\n",
      "the provincially destrica into which shareholders and communit of fund , the agreement in the prejud\n",
      "recommendment and function of a bill assed by the profession nation may be media , including and sta\n",
      "short a terms of the provincial system 1 commission with the disputters and than contempowers ros7ku\n",
      "( a ) ( si0.xTiGeL/ID(4dOy'u8\n",
      "software it may not specified in the nature of the secting our slegs 4-5Snq/\n",
      "interve and bioprospecify the Constitution of designed for transfer perform its as president pathoge\n",
      "the requipment are control official sectional legislations consible and , n-\n",
      "******* Model with N = 5 , alpha = 0.001 *******\n",
      "perplexity on the English train data:2.51123\n",
      "perplexity on the English dev data:2.78491\n",
      "** Generated text: **\n",
      "and province having , or modified in the biological repress who seek cates : \n",
      "1,1 ' -trichloroethyl fee mannexures\n",
      "( a ) the assigned as reference withing the commission , and all states of those in accordance and a\n",
      "the for the profession of the count have said in the applicategory .\n",
      "and 3 ) any applicable in the specified in the for the following any )\n",
      "po box / primary for a bill worldwide on the regulation 15 listerestribunal legislature of a state P\n",
      "mild symptoms must be rended by a municipality of the disease with the regulated by and three of any\n",
      "if a person a mixed non-carbon ) a bioproport and section of the services to it has a member ;\n",
      "the symptoms plus or person , or use culture that and when nation that the milling constitutional le\n",
      "( a ) phosphone or more that all the administerest simply must be declaration of the service presear\n",
      "******* Model with N = 5 , alpha = 0.0001 *******\n",
      "perplexity on the English train data:2.50690\n",
      "perplexity on the English dev data:2.82489\n",
      "** Generated text: **\n",
      "people with african be application , but when the americant in accordance . \n",
      "the symptoms in the nation , n-propyl ) -amined by the exploitation any informat of the ention of a \n",
      "( 2 ) of the indigenous bioprospecified in the preview of stanced to any controllection , the seats \n",
      "the for the required to a provincial legislational assembly and during a deputy province from 1 janu\n",
      "signed for .\n",
      "( 10 permit and any of the constitution ;\n",
      "capable of the princial legislation to make rules at the effect to the Assembly ; and control system\n",
      "participality , such a women part of nation in terms of the country ii ) that the in the systems spe\n",
      "and exploitation . \n",
      "( a ) the Minister to the define , in terms of the new consible in the council . \n",
      "******* Model with N = 5 , alpha = 1e-05 *******\n",
      "perplexity on the English train data:2.50646\n",
      "perplexity on the English dev data:2.87103\n",
      "** Generated text: **\n",
      "( 11 junin , and constitutional institution of business which all the address of the director any pe\n",
      "to provincial exercise of the follows :\n",
      "the control , except assembly , the above and designed by the improvincial legislature fillegal exer\n",
      "report or modified in the contact developmentation in the decide ) , and the systems specified in th\n",
      "a permit and act of duly , included in a resent the minister . \n",
      "this a commissioner , or a provincial development unlawful , in and the of the Premier of the admini\n",
      "prof justic services\n",
      "in the minister the nation is may case of any other the nation any other election . \n",
      "mr mission any sitting postal and must be utilised structions in an institution \n",
      "software constitutions of the provinces including permit is a women in a manufacture axis agreement \n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "lm = eng_lms[N]\n",
    "for alpha in [1, .01, .001, .0001, 1e-5]:\n",
    "    lm.alpha = alpha\n",
    "    print(\"******* Model with N =\", N, \", alpha =\", alpha, \"*******\")\n",
    "    train_ppl = lm.perplexity(corpora['eng']['train']['text']) # fix this to compute the perplexity on the English training data\n",
    "    dev_ppl = lm.perplexity(corpora['eng']['test']['text']) # fix this to compute the perplexity on the English development data\n",
    "    print(f\"perplexity on the English train data:{train_ppl:.5f}\")\n",
    "    print(f\"perplexity on the English dev data:{dev_ppl:.5f}\")\n",
    "    print(\"** Generated text: **\")\n",
    "    for _ in range(10):\n",
    "        print(generate(lm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba63c45c-4247-4157-826c-fd6ac44dc03c",
   "metadata": {},
   "source": [
    "👉 **THINK:** We found the best value of $\\alpha$ for a particular value of $N$, on the English data. Do you think the same value of $\\alpha$ will be best for other values of $N$?  What about on the other languages?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efac111-7cb9-4b1a-ace2-c77a1e43751d",
   "metadata": {},
   "source": [
    "## 7. Optimizing the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c838c5-9f76-412d-af4e-cab26b8933bd",
   "metadata": {},
   "source": [
    "Now let's be more systematic about finding the best values for $N$ and $\\alpha$. We will use a *grid search*, which just means we will loop over all possible pairs of values and pick the hyperparameters that have the lowest perplexity on the development set. \n",
    "\n",
    "Grid search is simple to implement, but can be very inefficient if the model needs to be re-trained for many hyperparameters or many possible values of each. \n",
    "\n",
    "In this case, we only need to train models for each value of $N$, because we can change the model's smoothing parameter without re-training. (This might not be true for all smoothing methods or all implementations of add-alpha smoothing, but it is true for ours!)\n",
    "\n",
    "We won't explore every possible option for $N$, but will look at a good range.\n",
    "\n",
    "👉 Run the next cell to find the hyperparameters that work best on our dev sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fedaae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching alphas with N = 3 for eng\n",
      "Searching alphas with N = 5 for eng\n",
      "Searching alphas with N = 8 for eng\n",
      "Searching alphas with N = 10 for eng\n",
      "Searching alphas with N = 15 for eng\n",
      "Best (N, alpha) for eng: (10, 0.001) with perplexity: 1.902814803906426\n",
      "Searching alphas with N = 3 for xho\n",
      "Searching alphas with N = 5 for xho\n",
      "Searching alphas with N = 8 for xho\n",
      "Searching alphas with N = 10 for xho\n",
      "Searching alphas with N = 15 for xho\n",
      "Best (N, alpha) for xho: (5, 0.01) with perplexity: 4.714911567372958\n",
      "Searching alphas with N = 3 for afr\n",
      "Searching alphas with N = 5 for afr\n",
      "Searching alphas with N = 8 for afr\n",
      "Searching alphas with N = 10 for afr\n",
      "Searching alphas with N = 15 for afr\n",
      "Best (N, alpha) for afr: (10, 0.001) with perplexity: 1.758623454226586\n"
     ]
    }
   ],
   "source": [
    "def tune_hyperps(lang, corpus, verbose = False):\n",
    "    '''Tunes the hyperparameters N and alpha by doing a grid search, \n",
    "    training on the train portion of corpus, and choosing the N and alpha\n",
    "    that have the lowest perplexity on the dev portion of corpus.\n",
    "    lang is a string identifying the  language of corpus.\n",
    "    If verbose = True, prints out perplexities of all models tested.'''\n",
    "    best_alpha = None\n",
    "    best_lm = None\n",
    "    best_n = None\n",
    "    best_prpl = float('inf')\n",
    "\n",
    "    for N in [3, 5, 8, 10, 15]:\n",
    "        lm = CharNGramLM(N=N)\n",
    "        lm.train(corpus['train']['text'])\n",
    "        print(f\"Searching alphas with N = {N} for {lang}\")\n",
    "        for alpha in [1, 0.1, 0.01, 0.001, 0.0001, 1e-05]:\n",
    "            lm.alpha = alpha\n",
    "            prpl = lm.perplexity(corpus['test']['text'])\n",
    "            if verbose: print(f'{alpha} {prpl}')\n",
    "            if prpl < best_prpl:\n",
    "                best_prpl = prpl\n",
    "                best_alpha = alpha\n",
    "                best_n = N\n",
    "                best_lm = lm # gives a pointer to this lm (not a copy)\n",
    "    print(f\"Best (N, alpha) for {lang}: ({best_n}, {best_alpha}) with perplexity: {best_prpl}\")\n",
    "    best_lm.alpha = best_alpha\n",
    "    return best_lm\n",
    "\n",
    "lms_tuned = {} # this will store the best models for each language\n",
    "for lang, corpus in corpora.items():\n",
    "    lms_tuned[lang] = tune_hyperps(lang, corpus, False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5053e6",
   "metadata": {},
   "source": [
    "Notice that the best models for English and Afrikaans are based on very long character strings (10-grams), and also have very low perplexity. (Remember that log(ppl) is roughly the number of bits of uncertainty per character, so on average each character has fewer than 1 bit of uncertainty --- less than a fair coin flip.)\n",
    "\n",
    "👉 **THINK:** What might this tell you about the English and Afrikaans datasets, as compared to the Xhosa one?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03be2ec5-ea74-4050-92b4-f5036e01e3b7",
   "metadata": {},
   "source": [
    "## 8. Language identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf6d8cf-5f61-49b1-a924-cbd6cbbc4e51",
   "metadata": {},
   "source": [
    "Now that we've tuned the hyperparameters, we are ready to try using our models for language identification! We will do this using the three files UDHR.1, UDHR.2, and UDHR.3, which contain the same text (the Universal Declaration of Human Rights) in each of the three languages. These are our *test sets*, since we are using them to see how well our models perform on data that we have not used for tuning.\n",
    "\n",
    "👉 Run the code below to compute the perplexity of each document under each of the three optimized models, then consider the following questions:\n",
    "\n",
    "1. According to the model perplexities, which language is each document most likely to be written in? Did the models identify the languages correctly? (You can open each document to check.)\n",
    "2. For each model, compare its perplexity on the development set (from the previous section) to its perplexity on the test set from the same language. Do you notice anything surprising? If so, do you have any possible explanations for this surprising result?\n",
    "3. These test documents are quite long, but many texts aren't. Given the results you've seen here and elsewhere in the lab, which of these models do you think is most likely to work well for language ID on short documents? Why?\n",
    "4. Suppose you had more time to work on this problem. In light of your answers to the previous questions, do you have any thoughts about how you could either *check* whether your explanations are correct, or *improve* the models to work better for language ID?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b9e67e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Testing on UDHR.1 ##\n",
      "Average perplexity for eng on UDHR.1: 69.11029918391465\n",
      "Average perplexity for xho on UDHR.1: 5.124435874453004\n",
      "Average perplexity for afr on UDHR.1: 69.12649161799182\n",
      "## Testing on UDHR.2 ##\n",
      "Average perplexity for eng on UDHR.2: 64.55076353151597\n",
      "Average perplexity for xho on UDHR.2: 92.37990005136078\n",
      "Average perplexity for afr on UDHR.2: 25.243713666375154\n",
      "## Testing on UDHR.3 ##\n",
      "Average perplexity for eng on UDHR.3: 24.092674750278576\n",
      "Average perplexity for xho on UDHR.3: 60.8512745179334\n",
      "Average perplexity for afr on UDHR.3: 60.24122680362763\n"
     ]
    }
   ],
   "source": [
    "# test on the UDHR\n",
    "for test_set in ['UDHR.1', 'UDHR.2', 'UDHR.3']:\n",
    "    print(f\"## Testing on {test_set} ##\")\n",
    "    with open(test_set) as f:\n",
    "        udhr_text = f.readlines()\n",
    "        udhr_text = [line.strip() for line in udhr_text if line.strip()] # remove empty lines\n",
    "        for lang, lm in lms_tuned.items():\n",
    "            lm = lms_tuned[lang]\n",
    "            perplexity = lm.perplexity(udhr_text)\n",
    "            print(f\"Average perplexity for {lang} on {test_set}: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d782bd1-92dd-4fec-99cf-02200d01c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xho数值过低，过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a661055-425f-40e9-9837-e911f8843848",
   "metadata": {},
   "source": [
    "## &#127881; &#127881; Congratulations! You're done! (Or just getting started?) &#x1F680; \n",
    "\n",
    "We hope you enjoyed exploring N-gram models and language ID!\n",
    "\n",
    "We wanted to give you a sense for how choices about data can make a big difference to the performance of an NLP system, in ways that are not always obvious at the beginning. Developing a system is often a very iterative process. \n",
    "\n",
    "Of course, now that you have found some problems with the setup we used here, and thought of ideas about how you could improve it, you might want to actually try some of those! Again, this is totally optional, but if you do any of that exploration and find something interesting, we'd love to hear about it on Piazza!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
