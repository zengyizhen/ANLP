{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb1b3c7",
   "metadata": {},
   "source": [
    "# Lab 2: Language Identification with character n-gram models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47003a10-0782-4c77-a8ee-b7a491533084",
   "metadata": {},
   "source": [
    "## Character n-gram models: what and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f8813e-8562-424a-be7c-73838bdeb47c",
   "metadata": {},
   "source": [
    "In lecture, we talked about n-gram models over **words**, but it's also possible to build n-gram models over **characters**. These can actually be useful for certain tasks, like identifying what language a text is written in.\n",
    "\n",
    "It's much easier (and requires less data) to build an n-gram model over characters than over words, so will do that here, to help you build your intuitions about this type of model.\n",
    "\n",
    "You will work with data from three different languages to build and explore character-level n-gram models for a simple language identification task. Along the way, youâ€™ll confront issues like rare characters, smoothing, underflow, and generation strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e54d4c-a288-40c2-8a8e-6c073eda9afa",
   "metadata": {},
   "source": [
    "## What you will learn in this lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca3df46-c9ef-46eb-abae-5ae94e12bee3",
   "metadata": {},
   "source": [
    "### Tools and practical issues: \n",
    "\n",
    "In this lab, you will learn:\n",
    "- how to easily split data into training and development subsets using the `datasets` library.\n",
    "- how to use these splits, together with a separate test set, to correctly to tune hyperparameters and test generalization.\n",
    "- some possible pitfalls to watch out for in your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba068a0-0593-42cc-9131-668e31ffe758",
   "metadata": {},
   "source": [
    "### Concepts: n-gram models and language identification\n",
    "\n",
    "After working through the lab, you should be able to:\n",
    "- compute the probability of a sequence given an n-gram model\n",
    "- explain how to generate new sentences using an n-gram language model\n",
    "- explain how to use a character n-gram model to do language identification\n",
    "   \n",
    "You should also understand more clearly:\n",
    "- how n-gram models are trained\n",
    "- the effects of smoothing and other hyperparameter choices on the model's behaviour (both generation and perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ebce01",
   "metadata": {},
   "source": [
    "## 1. Loading and splitting the data\n",
    "\n",
    "Today youâ€™ll use another dataset we uploaded to Hugging Face. It contains sentences in three languages widely spoken in South Africa: \n",
    "- **English:** a language in the West Germanic branch of the Indo-European language family with significant influences from Old French. It is spoken as a first language by around 380 million people worldwide, with another billion second-language speakers.\n",
    "- **Afrikaans:** another language in the West Germanic branch of the Indo-European language family. It evolved from Dutch starting around the 17th century, and is spoken as a first language by around 7 million people, mainly in South Africa and nearby regions.\n",
    "- **Xhosa: (or isiXhosa):** a language in the Nguni branch of the Bantu language family. It is closely related to Zulu and is spoken as a first language by around 8 million people, mainly in South Africa and nearby regions.\n",
    "\n",
    "ðŸ‘‰ Run the next two cells to load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3f0041-9cef-4401-86b2-0024a4e311aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once again, we need to update `datasets`\n",
    "%pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed475497",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Our variable names are based on the ISO 639-3 language codes for the languages in the dataset.\n",
    "# These are common language codes used in NLP, but you may also see two-character ISO 639-1 codes commonly.\n",
    "# However, ISO 639-3 is more comprehensive and includes many languages not covered by the two-character codes!\n",
    "eng = load_dataset('EdinburghNLP/south-african-lang-id', 'english')['train']\n",
    "xho = load_dataset('EdinburghNLP/south-african-lang-id', 'xhosa')['train']\n",
    "afr = load_dataset('EdinburghNLP/south-african-lang-id', 'afrikaans')['train']\n",
    "\n",
    "# By using a dictionary, you can associate each dataset with its language code.\n",
    "# Do you see why we use the code as the key and not the dataset itself?\n",
    "# This allows to write code that can operate on any number of languages without changing the code structure.\n",
    "# And it makes it easy to keep track of which dataset corresponds to which language.\n",
    "# This is a common pattern in NLP and other data processing tasks.\n",
    "corpora = {'eng': eng, 'xho': xho, 'afr': afr}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565bbb49",
   "metadata": {},
   "source": [
    "In this lab, you'll be training models, adjusting hyperparameters, and evaluating them. To evaluate your changes and avoid over-fitting, you'll use the *train--dev--test* splitting paradigm discussed in class. \n",
    "\n",
    "You might have noticed that when we loaded the dataset from HuggingFace, we selected a key called `'train'`. That's because you always have to select a split before you can iterate over a HuggingFace dataset, and HuggingFace interprets data without any splits as being a single `'train'` split. \n",
    "\n",
    "However, we are now going to split this data to create the actual training and development sets, so that you can explore and tune hyperparameters on the development set. \n",
    "\n",
    "The `datasets` library has a function we can use to easily split off a smaller subset of the data for development, but note that it will call this subset `'test'`!! Despite the name, this split will *not* be used for the test set in this lab. We have set aside another dataset for that, which we will introduce toward the end of the lab.\n",
    "\n",
    "By default, `train_test_split` selects a *random* subset of the data to split off, which can be useful if there might be differences between the early and late parts of the dataset, and you don't want this to affect your training. However, if you're trying to compare directly to (your own or someone else's) previous work, you need to be careful about whether you split the data the same way. \n",
    "\n",
    "**Further reading for later:** If you're new to Hugging Face, they provide useful tutorials with much more information about Datasets and other libraries, which you can find [here](https://huggingface.co/docs/datasets/index).\n",
    "\n",
    "ðŸ‘‰ Run the next two cells to load the data and see what its structure is. Then, change the final line in the second cell to print out the first 10 sentences in each language. Do you see anything that surprises you? (You might or might not, remember the data splits are random!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e8ae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = {\n",
    "    lang: dataset.train_test_split(test_size=0.1) # this puts 10% of the data into a \"test\" set (which we will use for development)\n",
    "    for lang, dataset in corpora.items() # see how organizing the data in a dictionary allows us to write less code?\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e985ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a quick look at the data.\n",
    "for lang, dataset in corpora.items():\n",
    "    print(f'### {lang} data: ###')\n",
    "    # This line should help you see the structure of the datasets.\n",
    "    # Replace it with code to print the first ten lines from the training set of each language!\n",
    "    print(\"FIX ME!\", dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d4806a-1a02-476a-ab91-7e367ce732b2",
   "metadata": {},
   "source": [
    "## 2. Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be979181-102a-4703-96be-f0d43abf79f6",
   "metadata": {},
   "source": [
    "If we directly train models on this data and try to evaluate their perplexity on held out data, we might encounter errors. That's because there may be rare characters that we don't see in training. (This would be a much bigger problem if our model was over words, but can be a problem even for characters.) We want our code to generalize to unseen characters, so we can still assign probabilities to sequences that contain them. \n",
    "\n",
    "We will take a standard approach and replace all rare and unseen characters with a shared unknown character: `ï¿½` (the Unicode character for an unknown character). This allows our model to assign some probability to unseen characters and also prevents it overfitting to rare characters which are coincidentally only present in one of the corpora. \n",
    "\n",
    "ðŸ‘‰ **THINK:** What does this imply about the probabilities of different rare or unseen characters? How is it similar to add-alpha smoothing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18948761-c260-4031-84a7-ba9a392ed2bd",
   "metadata": {},
   "source": [
    "Some languages do contain characters that don't occur in others! For instance, English doesn't use `à¤•`, `Ã£` or `è§’`.\n",
    "\n",
    "ðŸ‘‰ **THINK:** Do you think an English n-gram model *should* assign zero probability to strings containing these characters? Why or why not? Can you think of any contexts where they might appear in English text?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97de7e1-15d4-4459-8ab6-640890bd213e",
   "metadata": {},
   "source": [
    "In order to remove rare characters, we need to decide what counts as \"rare\". Normally, you would take a look at your data statistics and perhaps try a few different frequency thresholds (tuning on a development set).\n",
    "\n",
    "In this case, we've done some of that for you already and we are just going to use a threshold of 500. But you should still sanity-check the results by looking at what is getting removed.\n",
    "\n",
    "ðŸ‘‰ Run the next two cells to identify the rare characters and print them out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ee13ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def find_rare_chars(corpora, threshold=500):\n",
    "    \"\"\" find characters that occur fewer than 'threshold' times across all corpora\n",
    "    and return them as an alphanumerically sorted string \"\"\"\n",
    "\n",
    "    # count character frequencies in each corpus\n",
    "    counters  = []\n",
    "    for _, corpus in corpora.items():\n",
    "        counts = Counter()\n",
    "        for text in corpus['train']:\n",
    "            counts.update(text['text'])\n",
    "        counters.append(counts)\n",
    "\n",
    "    all_chars = set([char for counter in counters for char in counter.keys()])\n",
    "\n",
    "    # characters that occur less than 500 times across all three texts:\n",
    "    rare_chars = [char for char in all_chars if sum([freq[char] for freq in counters]) < threshold]\n",
    "    return ''.join(sorted(rare_chars)) # sorting alphnumerically to make output a bit easier to scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbde31dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_chars = find_rare_chars(corpora)\n",
    "print(rare_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b007266d-60b7-43b1-b615-acc44f54d56f",
   "metadata": {},
   "source": [
    "ðŸ‘‰ **THINK:** Look at the rare characters. Do you think any of these characters shouldn't be replaced with the unknown character? If so, why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5343447d-be9b-4eda-92e1-16f0a8f06889",
   "metadata": {},
   "source": [
    "ðŸ‘‰ **CHALLENGE QUESTION:** We know that encountering unseen characters in test data will cause problems. But why don't we simply deal with that at test time, by replacing the unseen characters with `ï¿½`? That is, why did we use `ï¿½` to replace characters that *did* occur in training, but rarely? (There are actually a few reasons, some more subtle than others. Don't spend more than a couple of minutes thinking about this now, you can come back to it later.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751f1dd5",
   "metadata": {},
   "source": [
    "ðŸ‘‰ Now, run the next cell to actually replace the rare characters with the unknown character using a regular expression. (You'll learn more about these in CPSLP if you're taking it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0794b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "corpora = {\n",
    "\tlang: corpus.map(\n",
    "\t\tlambda x: {'text': re.sub(f\"[{re.escape(rare_chars)}]\", 'ï¿½', x['text'])}, # This regular expression identifies any character in rare_chars so we can replace it with the unknown character\n",
    "\t)\n",
    "\tfor lang, corpus in corpora.items()\n",
    "}  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f8d660",
   "metadata": {},
   "source": [
    "## 3. Defining the n-gram model and looking at test cases\n",
    "\n",
    "Now that we've preprocessed our corpora, it's time to build our n-gram model. We've written it so that our model works for different values of $N$. It also implements add-alpha smoothing. \n",
    "\n",
    "ðŸ‘‰ **THINK:** Why do we need smoothing, even after replacing unknown characters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e765ce5-c5df-4426-a0ef-83628623d19d",
   "metadata": {},
   "source": [
    "The code for the n-gram model is below. It's probably not a good use of the lab session time to go through every line right now, but we encourage you to review it in more detail later. \n",
    "\n",
    "ðŸ‘‰ For now, just make sure you understand the role of each function at a high level. Then, run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c7f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class CharNGramLM:\n",
    "    def __init__(self, N=3, alpha=0.01):\n",
    "        self.N = N\n",
    "        if alpha < 0:\n",
    "            raise ValueError(\"Invalid value of alpha!\")\n",
    "        else:\n",
    "            self.alpha = alpha\n",
    "        # dictionary to hold counts of n-grams\n",
    "        # where keys are n-1 character tuples (context)\n",
    "        self.context_counts = defaultdict(\n",
    "            Counter\n",
    "        )  # a defautltdict allows us to create a new Counter for each new n-1-gram automatically\n",
    "        if N > 1:\n",
    "            self.vocab = set([\"<s>\", \"</s>\"])\n",
    "        elif N == 1:\n",
    "            self.vocab = set([\"</s>\"])  # don't need BOS\n",
    "        else:\n",
    "            raise ValueError(\"Invalid value of N!\")\n",
    "\n",
    "    def train(self, corpus):\n",
    "        ''' Given a corpus of sentences, store the counts of all character N-grams in the corpus.'''\n",
    "        for sentence in corpus:\n",
    "            # add start and end tokens\n",
    "            sentence = [\"<s>\"] * (self.N - 1) + list(sentence) + [\"</s>\"]\n",
    "            # update the counts of each n-gram\n",
    "            for i in range(len(sentence) - self.N + 1):\n",
    "                context = tuple(sentence[i : i + self.N - 1])\n",
    "                char = sentence[i + self.N - 1]\n",
    "                self.context_counts[context][char] += 1\n",
    "                self.vocab.add(char)\n",
    "\n",
    "    def print_counts(self):\n",
    "        ''' Print out the counts of all N-grams that have non-zero counts, alphnumerically ordered.'''\n",
    "        for context, counts in sorted(self.context_counts.items()):\n",
    "            print(f\"Context {context}:\")\n",
    "            for char, count in sorted(counts.items()):\n",
    "                print(f\"   C({char!r} | {context}) = {count}\")\n",
    "\n",
    "    def print_probs(self):\n",
    "        ''' For each context in alphanumeric order, print out the conditional probability\n",
    "        of each character in the vocabulary (including zero probabilities).'''\n",
    "        for context, counts in sorted(self.context_counts.items()):\n",
    "            print(f\"Context {context}:\")\n",
    "            for char in sorted(self.vocab):\n",
    "                if char != \"<s>\" : # We never generate <s> following another character, so it's not in the conditional probabilities\n",
    "                    prob = self.prob(context, char)\n",
    "                    print(f\"   P({char!r} | {context}) = {prob}\")\n",
    "\n",
    "    # We've included the next two functions so it's easier for you to check\n",
    "    # correctness on tiny examples, but we don't normally use raw\n",
    "    # probabilities in models because they can underflow for very long\n",
    "    # sequences. In this case, we do the computation in log space to\n",
    "    # make sure it will always be right, and this function should also\n",
    "    # work for reasonable sequence lengths.\n",
    "    def prob(self, context, char):\n",
    "        '''Returns the smoothed probability of char in the given context'''\n",
    "        return 2 ** self.logprob(context, char)\n",
    "\n",
    "    def prob_seq(self, sentence):\n",
    "        '''Returns the  probability of the sentence, according to the model'''\n",
    "        return 2 ** self.logprob_seq(sentence)\n",
    "\n",
    "    def logprob(self, context, char):\n",
    "        '''Returns the smoothed log probability of char in the given context'''\n",
    "        context = tuple(context)\n",
    "        counts = self.context_counts[context]\n",
    "        if self.N == 1:\n",
    "            V = len(self.vocab)\n",
    "        else:\n",
    "            V = len(self.vocab) - 1 # Only count characters we might generate next, which doesn't include <s>\n",
    "        if (self.alpha == 0) and (counts[char] == 0):\n",
    "            return -math.inf # Negative infinity\n",
    "        else:\n",
    "            prob = (counts[char] + self.alpha) / (sum(counts.values()) + self.alpha * V)\n",
    "            return math.log2(prob)\n",
    "\n",
    "    def logprob_seq(self, sentence):\n",
    "        '''Returns the log probability of the sentence, according to the model'''\n",
    "        sentence = [\"<s>\"] * (self.N - 1) + list(sentence) + [\"</s>\"]\n",
    "        # replace OOV characters with a placeholder\n",
    "        sentence = [char if char in self.vocab else \"ï¿½\" for char in sentence]\n",
    "        score = 0.0\n",
    "        for i in range(len(sentence) - self.N + 1):\n",
    "            context = tuple(sentence[i : i + self.N - 1])\n",
    "            char = sentence[i + self.N - 1]\n",
    "            score += self.logprob(context, char)  # add the log probs of each n-gram (multiply the raw probabilities)\n",
    "        return score\n",
    "\n",
    "    def perplexity(self, corpus):\n",
    "        '''Returns the perplexity of the model on the given corpus'''        \n",
    "        length = 0\n",
    "        log_prob_sum = 0.0\n",
    "        for sentence in corpus:\n",
    "            log_prob_sum += self.logprob_seq(sentence)\n",
    "            length += len(sentence) + 1  # account for </s>\n",
    "        return 2 ** (-log_prob_sum / length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f0fe5b-9d2c-4c7d-8b10-b1a42a59595d",
   "metadata": {},
   "source": [
    "Never assume that code is correct without testing it, whether it was written by you, us, or someone else (including AI)!\n",
    "\n",
    "Before you train the model on the real data, you should check that it works correctly on a very small test case where you can check the results by hand. This will also help ensure that *you* understand how to compute the right result.\n",
    "\n",
    "(It's not always possible to check results this way! Sometimes you'll need to think of other ways to test code, and ideally you can write automated test cases.)\n",
    "\n",
    "We've constructed one such test case below.\n",
    "\n",
    "ðŸ‘‰ Before you run this test case, compute the counts, probabilities, and perplexity by hand. Remember that for a sequence of length $L$, the perplexity is $2^{(-1/L)*log P(seq)}$. You'll need to consider how the begin/end of sequence markers figure into the computations! \n",
    "\n",
    "ðŸ‘‰ Now run the test code below to check that your answers match the output of our model. If they don't, is there a bug in the model or in your own understanding?\n",
    "\n",
    "ðŸ‘‰ We started by checking the unigram model without smoothing, because it's the simplest, but it's important to ensure the code also works for other cases. Again **by hand**, \n",
    "1. Figure out what counts and probabilities you should you get from this corpus if you use a bigram model (still with alpha = 0).\n",
    "2. Do you expect the training and testing perplexities to be higher or lower than with the unigram model? Why?\n",
    "\n",
    "ðŸ‘‰ Now set N=2 and re-run the test code to check your answers. Do you see where smoothing becomes necessary? Update the value of alpha, rerun the code, and check that this change does what you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f162269-5c8e-4a2d-9f27-751e8dad481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny toy corpus\n",
    "tiny_train = ['hi', 'ha', 'hi']\n",
    "tiny_test = ['hi','i']\n",
    "tiny_lm = CharNGramLM(N=1,alpha=0)\n",
    "tiny_lm.train(tiny_train)\n",
    "tiny_lm.print_counts()\n",
    "tiny_lm.print_probs()\n",
    "print(tiny_lm.prob_seq('hi'))\n",
    "print (f\"PPL on train: {tiny_lm.perplexity(tiny_train):.4}\")\n",
    "print (f\"PPL on test: {tiny_lm.perplexity(tiny_test):.4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3d84c5-b1bf-4037-8493-6f19dad02c76",
   "metadata": {},
   "source": [
    "## 4. Training on real corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67d0e1e-743f-4629-80b0-4a5f34f13b66",
   "metadata": {},
   "source": [
    "Character n-gram models can be used for language identification by training models on different languages. Then, when you get a piece of text, you check its perplexity under each model. Whichever model has the lowest perplexity is identified as the language of the text.\n",
    "\n",
    "Systems based on this idea are efficient and often work well, but they can still make errors! \n",
    "\n",
    "Let's take a look at what kinds of examples might present problems for this sort of model, using a simple trigram model with default smoothing.\n",
    "\n",
    "ðŸ‘‰ Run the two cells below to train models on each of our three languages and compute the perplexities of some test sentences that could occur in English. Is the perplexity always lowest for the English model? If not, what sorts of input seem to cause problems for this way of doing language ID?\n",
    "\n",
    "Feel free to add your own test sentences to the list to explore this question further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca78088-3188-44ea-840e-4a135ddb73f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a character n-gram language model for each language.\n",
    "lms = {}\n",
    "for lang, corpus in corpora.items():\n",
    "    lm = CharNGramLM(N=3)\n",
    "    lm.train(corpus['train']['text'])\n",
    "    lms[lang] = lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cabaff-aa26-440c-98eb-a7b85e97879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_test_sentences = ['I love natural language processing.',\n",
    "                     'An incomplete sentence.',\n",
    "                     'marketing and sales operations',\n",
    "                     'Pierre Vinken, chairman of Elsevier, is well-known in NLP for appearing in the first sentence of the WSJ corpus.',\n",
    "                     'Hi',\n",
    "                     'no',\n",
    "                     'See?',\n",
    "                     'Aha, Lycketoft.', # Lycketoft was one of the words in English Europarl with count=1\n",
    "                     '3601',\n",
    "                     'hey @sloppyjoe wassup #chillin #fridaynight']\n",
    "for sentence in my_test_sentences:\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    for lang, lm in lms.items():\n",
    "        print(f\"  {lang} ppl: {lm.perplexity([sentence]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f620a01-f6dc-4d1a-9733-aa4979c55cf5",
   "metadata": {},
   "source": [
    "## 5. Generating from language models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddba6ad-3ecc-4da8-9719-66c3bca4c902",
   "metadata": {},
   "source": [
    "This section is a small digression from the language ID task, to help you build intuitions about *generating* from language models. \n",
    "\n",
    "**If you are running short on time** and want to focus on language ID, you can skip this section and come back to it later. Before going to Section 4, you will need to **run the first two cells below.**\n",
    "\n",
    "Below, we've provided some code that implements several different generation (decoding) strategies. The default is just to sample from the language model probabilities (standard generation), but it also implements top-$k$ and temperature-scaled sampling. We will only explore top-$k$ sampling here, but if you want you can look at temperature-scaled sampling on your own after the lab.\n",
    "\n",
    "To better see how the output is affected by both $N$ and the sampling  method, we'll generate from models with different values of $N$.\n",
    "\n",
    "ðŸ‘‰ Run the next two cells to train English models with different values of $N$ and implement generation. Then scroll down to the next question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527114d4-5609-4dde-85d2-ba7aa3890c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train some models on English with different N\n",
    "eng_lms = {}\n",
    "for N in [1, 3, 5, 10]:\n",
    "    lm = CharNGramLM(N=N)\n",
    "    lm.train(corpora['eng']['train']['text'])\n",
    "    eng_lms[N] = lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500e617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def generate(model, top_k=0, temperature=1, max_len=100):\n",
    "    '''Given a language model, use it to generate a single sequence. \n",
    "    Generation will stop when </s> is generated, or after max_len chars (whichever comes first).\n",
    "    If top_k = 0, we sample from the full distribution, otherwise re-normalize the top k choices and sample from those.\n",
    "    '''\n",
    "    context = ['<s>'] * (model.N - 1)\n",
    "    result = []\n",
    "    for _ in range(max_len):\n",
    "        nlog_probs = []\n",
    "        chars = []\n",
    "        for c in model.vocab:\n",
    "            log_p = model.logprob(context, c) / temperature\n",
    "            nlog_probs.append(-log_p)\n",
    "            chars.append(c)\n",
    "        \n",
    "        if top_k > 0:\n",
    "            pairs = sorted(zip(chars, nlog_probs), key=lambda x: x[1])[:top_k]\n",
    "            chars, nlog_probs = zip(*pairs)\n",
    "            probs = [math.exp(-log_p) for log_p in nlog_probs]\n",
    "        else:\n",
    "            exp_probs = [math.exp(-log_p) for log_p in nlog_probs]\n",
    "            total = sum(exp_probs)\n",
    "            probs = [p / total for p in exp_probs]\n",
    "\n",
    "        next_char = random.choices(chars, weights=probs if top_k > 0 else exp_probs)[0]\n",
    "        if next_char == '</s>':\n",
    "            break\n",
    "        result.append(next_char)\n",
    "        context = context[1:] + [next_char]\n",
    "    return ''.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b2fc44-0fba-45ad-901b-6f4e3448a8fa",
   "metadata": {},
   "source": [
    "ðŸ‘‰ The following cell is going to generate 10 sequences (up to 100 characters each) from each of the four English $N$-gram models, which have $N$=\\[1, 3, 5, 10\\]. \n",
    "1. Roughly speaking, what do you think the difference will be between the output of the four models? Run the code to find out if you are correct.\n",
    "2. (**Challenge question**) Now take a closer look at the output. Do you see any places where the 5-gram or 10-gram model suddenly started generating total junk? (If not, you might need to rerun the code to get some new sequences. You should see this eventually.) Can you explain why that happened? \n",
    "3. So far we used $k=0$, which actually turns off top-k sampling (so, we sample from the full distribution when generating each character). What do you think will happen if you change $k$ to be 1, and how might that change the output of each model? (*Hint*: What character will be chosen at each generation step?) Try it to see if you're right!\n",
    "4. If you want, you can try other values of $k$ to see whether you can find a good balance between generating diverse outputs and not generating junk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdc8570-6252-478c-950d-903a418a7e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "for N, lm in eng_lms.items():\n",
    "    print(\"******* Generating with N =\", N, \", k =\", k, \"*******\")\n",
    "    for _ in range(10):\n",
    "        print(generate(lm, top_k=k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eab39da-ef82-4586-a3df-90a39dcf0533",
   "metadata": {},
   "source": [
    "ðŸ‘‰ **THINK:** If we want to get the best performance from our language ID system, we will need to tune the model hyperparameters (as we'll do in the next section). Should we include $k$ in the hyperparameters that we tune? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab6c7ba-fcec-4ad0-a391-3dbd1966f1ee",
   "metadata": {},
   "source": [
    "## 6. Exploring the effects of $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3a1568-ab93-4928-affe-b45c4c56a8fa",
   "metadata": {},
   "source": [
    "Now, let's see how the smoothing parameter affects the generated text and the model perplexity.\n",
    "\n",
    "For generation, we  will use regular sampling ($k=0$). Our code uses a 5-gram model, but you can try other values of $N$ later if you want.\n",
    "\n",
    "Notice that we are exploring $\\alpha$ values over several orders of magnitude. This is a common pattern in NLP and machine learning, especially if you don't have a good idea of the range that will work well. If you find huge differences between the results, you can always try more values in between the big jumps.\n",
    "\n",
    "ðŸ‘‰ Run the code and look at the generated output. \n",
    "1. How does the value of alpha affect the generation quality?\n",
    "2. Will the alpha that yields the best quality output also have the lowest perplexity on the development set? Why or why not?\n",
    "3. Now fill in the code to actually compute the perplexities on the training and development parts of the English data. Was your prediction correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60df0552-7149-4851-9f2a-4412c321ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "lm = eng_lms[N]\n",
    "for alpha in [1, .01, .001, .0001, 1e-5]:\n",
    "    lm.alpha = alpha\n",
    "    print(\"******* Model with N =\", N, \", alpha =\", alpha, \"*******\")\n",
    "    train_ppl = -1 # fix this to compute the perplexity on the English training data\n",
    "    dev_ppl = -1 # fix this to compute the perplexity on the English development data\n",
    "    print(f\"** Perplexities: FIX ME TO PRINT OUT THE TRAIN AND DEV PERPLEXITIES **\")\n",
    "    print(\"** Generated text: **\")\n",
    "    for _ in range(10):\n",
    "        print(generate(lm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba63c45c-4247-4157-826c-fd6ac44dc03c",
   "metadata": {},
   "source": [
    "ðŸ‘‰ **THINK:** We found the best value of $\\alpha$ for a particular value of $N$, on the English data. Do you think the same value of $\\alpha$ will be best for other values of $N$?  What about on the other languages?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efac111-7cb9-4b1a-ace2-c77a1e43751d",
   "metadata": {},
   "source": [
    "## 7. Optimizing the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c838c5-9f76-412d-af4e-cab26b8933bd",
   "metadata": {},
   "source": [
    "Now let's be more systematic about finding the best values for $N$ and $\\alpha$. We will use a *grid search*, which just means we will loop over all possible pairs of values and pick the hyperparameters that have the lowest perplexity on the development set. \n",
    "\n",
    "Grid search is simple to implement, but can be very inefficient if the model needs to be re-trained for many hyperparameters or many possible values of each. \n",
    "\n",
    "In this case, we only need to train models for each value of $N$, because we can change the model's smoothing parameter without re-training. (This might not be true for all smoothing methods or all implementations of add-alpha smoothing, but it is true for ours!)\n",
    "\n",
    "We won't explore every possible option for $N$, but will look at a good range.\n",
    "\n",
    "ðŸ‘‰ Run the next cell to find the hyperparameters that work best on our dev sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fedaae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperps(lang, corpus, verbose = False):\n",
    "    '''Tunes the hyperparameters N and alpha by doing a grid search, \n",
    "    training on the train portion of corpus, and choosing the N and alpha\n",
    "    that have the lowest perplexity on the dev portion of corpus.\n",
    "    lang is a string identifying the  language of corpus.\n",
    "    If verbose = True, prints out perplexities of all models tested.'''\n",
    "    best_alpha = None\n",
    "    best_lm = None\n",
    "    best_n = None\n",
    "    best_prpl = float('inf')\n",
    "\n",
    "    for N in [3, 5, 8, 10, 15]:\n",
    "        lm = CharNGramLM(N=N)\n",
    "        lm.train(corpus['train']['text'])\n",
    "        print(f\"Searching alphas with N = {N} for {lang}\")\n",
    "        for alpha in [1, 0.1, 0.01, 0.001, 0.0001, 1e-05]:\n",
    "            lm.alpha = alpha\n",
    "            prpl = lm.perplexity(corpus['test']['text'])\n",
    "            if verbose: print(f'{alpha} {prpl}')\n",
    "            if prpl < best_prpl:\n",
    "                best_prpl = prpl\n",
    "                best_alpha = alpha\n",
    "                best_n = N\n",
    "                best_lm = lm # gives a pointer to this lm (not a copy)\n",
    "    print(f\"Best (N, alpha) for {lang}: ({best_n}, {best_alpha}) with perplexity: {best_prpl}\")\n",
    "    best_lm.alpha = best_alpha\n",
    "    return best_lm\n",
    "\n",
    "lms_tuned = {} # this will store the best models for each language\n",
    "for lang, corpus in corpora.items():\n",
    "    lms_tuned[lang] = tune_hyperps(lang, corpus, False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5053e6",
   "metadata": {},
   "source": [
    "Notice that the best models for English and Afrikaans are based on very long character strings (10-grams), and also have very low perplexity. (Remember that log(ppl) is roughly the number of bits of uncertainty per character, so on average each character has fewer than 1 bit of uncertainty --- less than a fair coin flip.)\n",
    "\n",
    "ðŸ‘‰ **THINK:** What might this tell you about the English and Afrikaans datasets, as compared to the Xhosa one?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03be2ec5-ea74-4050-92b4-f5036e01e3b7",
   "metadata": {},
   "source": [
    "## 8. Language identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf6d8cf-5f61-49b1-a924-cbd6cbbc4e51",
   "metadata": {},
   "source": [
    "Now that we've tuned the hyperparameters, we are ready to try using our models for language identification! We will do this using the three files UDHR.1, UDHR.2, and UDHR.3, which contain the same text (the Universal Declaration of Human Rights) in each of the three languages. These are our *test sets*, since we are using them to see how well our models perform on data that we have not used for tuning.\n",
    "\n",
    "ðŸ‘‰ Run the code below to compute the perplexity of each document under each of the three optimized models, then consider the following questions:\n",
    "\n",
    "1. According to the model perplexities, which language is each document most likely to be written in? Did the models identify the languages correctly? (You can open each document to check.)\n",
    "2. For each model, compare its perplexity on the development set (from the previous section) to its perplexity on the test set from the same language. Do you notice anything surprising? If so, do you have any possible explanations for this surprising result?\n",
    "3. These test documents are quite long, but many texts aren't. Given the results you've seen here and elsewhere in the lab, which of these models do you think is most likely to work well for language ID on short documents? Why?\n",
    "4. Suppose you had more time to work on this problem. In light of your answers to the previous questions, do you have any thoughts about how you could either *check* whether your explanations are correct, or *improve* the models to work better for language ID?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9e67e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on the UDHR\n",
    "for test_set in ['UDHR.1', 'UDHR.2', 'UDHR.3']:\n",
    "    print(f\"## Testing on {test_set} ##\")\n",
    "    with open(test_set) as f:\n",
    "        udhr_text = f.readlines()\n",
    "        udhr_text = [line.strip() for line in udhr_text if line.strip()] # remove empty lines\n",
    "        for lang, lm in lms_tuned.items():\n",
    "            lm = lms_tuned[lang]\n",
    "            perplexity = lm.perplexity(udhr_text)\n",
    "            print(f\"Average perplexity for {lang} on {test_set}: {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a661055-425f-40e9-9837-e911f8843848",
   "metadata": {},
   "source": [
    "## &#127881; &#127881; Congratulations! You're done! (Or just getting started?) &#x1F680; \n",
    "\n",
    "We hope you enjoyed exploring N-gram models and language ID!\n",
    "\n",
    "We wanted to give you a sense for how choices about data can make a big difference to the performance of an NLP system, in ways that are not always obvious at the beginning. Developing a system is often a very iterative process. \n",
    "\n",
    "Of course, now that you have found some problems with the setup we used here, and thought of ideas about how you could improve it, you might want to actually try some of those! Again, this is totally optional, but if you do any of that exploration and find something interesting, we'd love to hear about it on Piazza!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
