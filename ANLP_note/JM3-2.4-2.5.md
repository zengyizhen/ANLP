# Speech and Language Processing

James H. Martin

## Chapter 1: Large Language Models


Week 1: JM3 *2.0-2.1(\*)*, 2.6 (\*), 2.7, 17.1, *2.2 (\*)*, 2.3, *2.4 (\*)*

---

### 2.4 Subword Tokenization: Byte-Pair Encoding
**Tokenization**: input text-->tokens (words, morphemes, characters)

#### 2.4.1 BPE training: Learner mode (initialization)

**Parameter**: **k** is a parameter of the algorithm. The resulting vocabulary consists of the original set of characters plus k new symbols

**Features**: 
- this algorithm DOES NOT merge across word boundaries. i.e. seperated at white space and punctuation (regular expressions).
- white space usually attached the start of the word


>**function** BYTE-PAIR ENCODING(strings C, number of merges k)
>
>**returns** vocab V
>
>
>V ← all unique characters in C # initial set of tokens is characters
>
>**for** i = 1 **to** k **do** # merge tokens k times
>
>        tL, tR ← Most frequent pair of adjacent tokens in C
>
>        tNEW ← tL + tR # make new token by concatenating
>
>        V ← V + tNEW # update the vocabulary
>
>        Replace each occurrence of tL, tR in C with tNEW # and update the corpus
>
>**return** V*


#### 2.4.2 BPE: segmenter mode (encoder)
- Input: new corpus C and an ordered list of merges (from learner mode).
    - Split C into characters after adding the end-of-word symbol.
    - Apply merges to C in the order they were learned. 
- Note that segmentation does not care about frequencies in C.
- In real settings BPE is run with tens of thousands of merges on a very
large input corpus

#### 2.4.3 BPE: segmenter mode (encoder)
[OpenAIGPT4o Tokenizer](https://tiktokenizer.vercel.app/)
**pretokenization**:
**SuperBPE**: SuperBPE algorithm ﬁrstinduces regularBPE subword tokens by enforcing pretokenization.It then runs a second stage of BPE allowing merges across spaces and punctuation.

# 2.5 Rule-based tokenization
#### Penn Treebank tokenization
It needs to be very fast. For rule-based word tokenization we generally use **deterministic algorithms** based on regular expressions compiled into efficient **finite state automata**.
#### Regular Expression Tokenization in NLTK(Natural Language Toolkit)

##### Tokenization Process
The regex pattern used for tokenization includes:

- **Abbreviations**: Matches sequences of capital letters followed by a dot (e.g., U.S.A.)
- **Words with Hyphens**: Matches words that may contain internal hyphens (e.g., poster-print)
- **Currency and Percentages**: Matches currency values (with optional dollar sign) and percentages (e.g., $12.40, 82%)
- **Ellipses**: Identifies ellipses in text.
- **Punctuation Marks**: Recognizes separate tokens like punctuation marks.

**Example**
```python
>>> text = ’That U.S.A. poster-print costs $12.40...’
>>> pattern = r’’’(?x) # set flag to allow verbose regexps
... (?:[A-Z]\.)+ # abbreviations, e.g. U.S.A.
... | \w+(?:-\w+)* # words with optional internal hyphens
... | \$?\d+(?:\.\d+)?%? # currency, percentages, e.g. $12.40, 82%
... | \.\.\. # ellipsis
... | [][.,;"’?():_‘-] # these are separate tokens; includes ], [
... 
>>> nltk.regexp_tokenize(text, pattern)
[’That’, ’U.S.A.’, ’poster-print’, ’costs’, ’$12.40’, ’...’]
'''

