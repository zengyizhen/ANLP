{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "128a4adf-08ef-457c-b984-976c9dfb8690",
   "metadata": {},
   "source": [
    "# RNN, LSTM, GRU\n",
    "\n",
    "## Architecture\n",
    "\n",
    "- SRN\n",
    "- Stacked RNN\n",
    "\n",
    "**motivation**：SRN只能捕捉浅层信息\n",
    "\n",
    "What：上一层的输出作为下一层RNN的输入\n",
    "\n",
    "Result：表现比单层好，捕捉不同程度的抽象\n",
    "\n",
    "但，training cost训练成本增长的非常快\n",
    "\n",
    "- Bidirectional RNN\n",
    "\n",
    "**Motivation**：SRN只能捕捉当前timestamp之前的序列信息，而双向的RNn可以捕捉右侧的sequence\n",
    "\n",
    "what：一个SRN：RNNforward，使用反向序列训练一个RNNbackword，concatenation（逐元素的加法或者乘法） vector\n",
    "\n",
    "Result：序列分类sequence classification（相比于使用最后一层hidden+FFN+softmax——包含更多句子的尾部信息，使用两个方向RNN的最后一个hidden层结合）\n",
    "\n",
    "- LSTM\n",
    "\n",
    "[https://www.projectpractical.com/long-short-term-memory-lstm-interview-questions-and-answers/#:~:text=This guide aims to provide a comprehensive overview,are designed to give you a rounded preparation](https://www.projectpractical.com/long-short-term-memory-lstm-interview-questions-and-answers/#:~:text=This%20guide%20aims%20to%20provide%20a%20comprehensive%20overview,are%20designed%20to%20give%20you%20a%20rounded%20preparation).\n",
    "\n",
    "适用于有时间序列、NLP的任务\n",
    "\n",
    "**motivation**：\n",
    "\n",
    "1. RNN 每个cell的输出中包含附近的信息会更多，对于远距离的信息捕捉不充分\n",
    "2. RNN梯度消失问题：RNN训练时，误差信号反向传播过程中，隐藏层会受到序列长度影响，多次反复乘法，导致梯度最后逐渐趋近于0\n",
    "\n",
    "思想：分解为子问题，remove拿走不需要的信息，add加入后续可能用到的信息\n",
    "\n",
    "解决方案：add context layer+gates（input + previous hidden + previous context layer）\n",
    "\n",
    "Gate：feedforward layer+sigmoid（0 or 1）+点乘运算（可理解为mask 1留 0删）\n",
    "\n",
    "1. Forget Gate：删除上下文中不再需要的信息 sigmoid     k(t)=C(t-1)上下文层@f(t)\n",
    "2. Add Gate：选择要加入到当前上下文的信息 tanh   j(t)=G(t)实际信息@i(t).   上下文C(t)=j(t)+k(t)\n",
    "3. Output gate：决定当前hidden层需要什么信息sigmoid.  h(t)=o(t)输出门@tanh(C(t))\n",
    "\n",
    "- GRU\n",
    "\n",
    "## Train\n",
    "\n",
    "BPTT：Backpropagation through time\n",
    "\n",
    "BPTT 随时间推移展开 RNN，将其视为一个[深度神经网络](https://zhida.zhihu.com/search?content_id=236056996&content_type=Article&match_order=1&q=%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NjA2NDE3MDgsInEiOiLmt7HluqbnpZ7nu4_nvZHnu5wiLCJ6aGlkYV9zb3VyY2UiOiJlbnRpdHkiLCJjb250ZW50X2lkIjoyMzYwNTY5OTYsImNvbnRlbnRfdHlwZSI6IkFydGljbGUiLCJtYXRjaF9vcmRlciI6MSwiemRfdG9rZW4iOm51bGx9.IkQ2uux9KCeHwo9jbs89unsGxHC1QFO0G0afPabkGls&zhida_source=entity)，在时间步长中具有共享权重。梯度是通过将误差传播回展开的网络来计算的。因此，RNN 可以更新其权重并从顺序数据中学习。\n",
    "\n",
    "- [ ]  梯度裁剪\n",
    "- [ ]  梯度爆炸\n",
    "- [x]  梯度消失\n",
    "\n",
    " \n",
    "\n",
    "## Application\n",
    "\n",
    "### NLP\n",
    "\n",
    "- LM ：输入词向量预测下一个词的概率分布\n",
    "\n",
    "损失函数 average cross-entropy，self-supervised，teacher forcing\n",
    "\n",
    "- Sequence Labeling（POS：part-of-speech）\n",
    "\n",
    "输出是type tag\n",
    "\n",
    "- Sequence Classification\n",
    "\n",
    "使用最后一层hidden（或者pooling每个hidden）+FFN+softmax\n",
    "\n",
    "- Generation\n",
    "\n",
    "sample\n",
    "\n",
    "- MT：machine translation\n",
    "\n",
    "Encoder-context-Decoder\n",
    "\n",
    "股票预测、音乐生成、\n",
    "\n",
    "## Link\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/665673989"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694e44d0-0917-41a9-9931-f9fc100b8e40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
