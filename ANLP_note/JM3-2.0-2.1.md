# Speech and Language Processing
James H. Martin
## Chapter 1: Large Language Models

Week 1: JM3 *2.0-2.1(\*)*, 2.6 (\*), 2.7, 17.1, 2.2 (\*), 2.3, 2.4 (\*)

### 2.0 Word and Tokens
**Tokenization**: the task of separating out or
tokenizing words and word parts from running text

**How to represent words**
- Unicode
- UTF-8
- morpheme
  
**The standard way to tokenize text**
- **Byte-Pair Encoding (BPE)**: This algorithm uses simple statistics of letter sequences to induce a vocabulary of subword tokens.
- All tokenization systems also depend on **regular expressions** as a processing step.

**Edit distance**: measures how similar
two words or strings are based on the number of edits (insertions, deletions, substitutions) it takes to change one string into the other. 

### 2.1 Word (How to define "word")


>I do uh main- mainly business data processing

 This **Utterance**  has two kinds of disfluencies.
- Fragment: main-.
- Fillers/filled pauses: um/uh.
  
    Fillers can be kept cause they may signal clause or idea(or a cue to speaker identification), so for speech recognition they are treated as regular words.

---
  
>They picnicked by the pool, then lay back on the grass and looked at the stars.

    ðŸ‘‰This sentence has 16 words if we donâ€™t count punctuation as words, 18 if we count punctuation. If we ignore punctuation, the picnic sentence has 14 types and 16 instances.

Large language models generally count **punctuation** as separate words. **Word types** are the number of distinct words in a corpus; if the set of words in the vocabulary
word instance is $V$, the **number of types** is the vocabulary size $|V|$.
**Word instances** are the total
number N of running words.



Notice **Capitalized problem**: Should *They* and *they* be the same word type?

---

#### **Orthographic problem** 
Chinese, Japanese and Thai don't have orthographic at all
##### ðŸ“Š Example: Chinese
**Features**: each character generally represents a single
unit of meaning (called a **morpheme**, introduced below) and is pronounceable as a
single syllable.  Deciding what counts as a word in Chinese is complex. 

- Chinese Treebank(3 words):  
>å§šæ˜ŽYaoMing
è¿›å…¥
reaches
æ€»å†³èµ›
finals
- 'Peking University' standard(5 words):  
>å§š
Yao
æ˜Ž
Ming
è¿›å…¥
reaches
æ€»
overall
å†³èµ›
finals
- Using characters
as the basic elements(7 words) works pretty wellðŸ‘ since characters are at a reasonable **semantic level** for most
applications
> å§š
Yao
æ˜Ž
Ming
è¿›
enter
å…¥
enter
æ€»
overall
å†³
decision
èµ›
game
---
#### **The number of words problem**
The number of words grows without bound
- function words: a, of, the
- content words: nouns, adjectives and verbs
---
#### **Result**
NLP models use smaller units called **subwords** that can be recombined to model new words that our model has never
seen before.   
**Next step**: Defining subwords (morphemes and characters)